{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Running Inference Step by Step\n",
    "\n",
    "This tutorial demonstrates how to run inference step-by-step, breaking down the `inference.run()` command into its constituent parts. The new inference structure follows 5 sequential phases:\n",
    "\n",
    "1. **Phase 1**: Pre-selection + Incoherent filtering (per bank, no threshold)\n",
    "2. **Phase 2**: Global cut (find global max, apply threshold across all banks)\n",
    "3. **Phase 3**: Extrinsic sample generation (using filtered intrinsic samples)\n",
    "4. **Phase 4**: Coherent inference (per bank)\n",
    "5. **Phase 5**: Post-processing and combination\n",
    "\n",
    "In this tutorial we'll:\n",
    "\n",
    "1. Create mock data and a small bank\n",
    "2. Select intrinsic samples based on chirp mass\n",
    "3. Demonstrate Phase 1 & 2: Incoherent filtering and global cut\n",
    "4. Visualize selected samples vs. original bank\n",
    "5. Demonstrate Phase 3: Draw extrinsic samples using filtered indices\n",
    "6. Demonstrate Phase 4: Perform coherent likelihood evaluations\n",
    "7. Demonstrate Phase 5: Standardize samples and create corner plots\n",
    "8. Finally, show how to use `inference.run()` for a streamlined process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
    "\n",
    "from cogwheel import data, gw_utils, gw_plotting, utils\n",
    "from cogwheel.posterior import Posterior\n",
    "from cogwheel.likelihood import RelativeBinningLikelihood\n",
    "from dot_pe import inference, waveform_banks, config\n",
    "from dot_pe.power_law_mass_prior import PowerLawIntrinsicIASPrior\n",
    "from dot_pe.utils import load_intrinsic_samples_from_rundir\n",
    "\n",
    "# Set up artifacts directory for all outputs\n",
    "ARTIFACTS_DIR = Path(\"./artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Mock Data and Small Bank\n",
    "\n",
    "We'll use the methods from tutorial 1 and 2 to create a bank and generate an injection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3-detector injection\n",
    "eventname = \"tutorial_inference_event\"\n",
    "event_data = data.EventData.gaussian_noise(\n",
    "    eventname=eventname,\n",
    "    detector_names=\"HLV\",\n",
    "    duration=120.0,\n",
    "    asd_funcs=[\"asd_H_O3\", \"asd_L_O3\", \"asd_V_O3\"],\n",
    "    tgps=0.0,\n",
    "    fmax=1600.0,\n",
    "    seed=20223001,\n",
    ")\n",
    "\n",
    "# Injection parameters\n",
    "chirp_mass = 20.0\n",
    "q = 0.7\n",
    "m1, m2 = gw_utils.mchirpeta_to_m1m2(chirp_mass, gw_utils.q_to_eta(q))\n",
    "\n",
    "injection_par_dic = dict(\n",
    "    m1=m1,\n",
    "    m2=m2,\n",
    "    ra=0.5,\n",
    "    dec=0.5,\n",
    "    iota=np.pi / 3,\n",
    "    psi=1.0,\n",
    "    phi_ref=12.0,\n",
    "    s1z=0.3,\n",
    "    s2z=0.3,\n",
    "    s1x_n=0.1,\n",
    "    s1y_n=0.2,\n",
    "    s2x_n=0.3,\n",
    "    s2y_n=-0.2,\n",
    "    l1=0.0,\n",
    "    l2=0.0,\n",
    "    tgps=0.0,\n",
    "    f_ref=50.0,\n",
    "    d_luminosity=2000.0,\n",
    "    t_geocenter=0.0,\n",
    ")\n",
    "\n",
    "# Inject signal\n",
    "event_data.inject_signal(injection_par_dic, \"IMRPhenomXPHM\")\n",
    "snr = np.sqrt(\n",
    "    2 * (event_data.injection[\"d_h\"] - 0.5 * event_data.injection[\"h_h\"]).sum()\n",
    ")\n",
    "print(f\"Injection SNR: {snr:.2f}\")\n",
    "\n",
    "# Save event data\n",
    "event_path = ARTIFACTS_DIR / f\"{eventname}.npz\"\n",
    "event_data.to_npz(filename=event_path, overwrite=True)\n",
    "print(f\"Saved event data to: {event_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bank for tutorial\n",
    "bank_dir = ARTIFACTS_DIR / \"bank\"\n",
    "bank_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bank_size = 2**16\n",
    "mchirp_min = 15\n",
    "mchirp_max = 30\n",
    "q_min = 0.2\n",
    "f_ref = 50.0\n",
    "seed = 777\n",
    "n_pool = 4\n",
    "blocksize = 4096\n",
    "approximant = \"IMRPhenomXPHM\"\n",
    "\n",
    "# Generate bank samples\n",
    "powerlaw_prior = PowerLawIntrinsicIASPrior(\n",
    "    mchirp_range=(mchirp_min, mchirp_max),\n",
    "    q_min=q_min,\n",
    "    f_ref=f_ref,\n",
    ")\n",
    "\n",
    "print(f\"Generating {bank_size:,} bank samples...\")\n",
    "bank_samples = powerlaw_prior.generate_random_samples(\n",
    "    bank_size, seed=seed, return_lnz=False\n",
    ")\n",
    "\n",
    "# Compute derived quantities and weights\n",
    "bank_samples[\"mchirp\"] = gw_utils.m1m2_to_mchirp(bank_samples[\"m1\"], bank_samples[\"m2\"])\n",
    "bank_samples[\"lnq\"] = np.log(bank_samples[\"m2\"] / bank_samples[\"m1\"])\n",
    "bank_samples[\"chieff\"] = gw_utils.chieff(\n",
    "    *bank_samples[[\"m1\", \"m2\", \"s1z\", \"s2z\"]].values.T\n",
    ")\n",
    "mchirp_values = bank_samples[\"mchirp\"].values\n",
    "bank_samples[\"log_prior_weights\"] = 1.7 * np.log(mchirp_values)\n",
    "\n",
    "# Save bank\n",
    "bank_columns = [\n",
    "    \"m1\",\n",
    "    \"m2\",\n",
    "    \"s1z\",\n",
    "    \"s1x_n\",\n",
    "    \"s1y_n\",\n",
    "    \"s2z\",\n",
    "    \"s2x_n\",\n",
    "    \"s2y_n\",\n",
    "    \"iota\",\n",
    "    \"log_prior_weights\",\n",
    "]\n",
    "samples_path = bank_dir / \"intrinsic_sample_bank.feather\"\n",
    "bank_samples[bank_columns].to_feather(samples_path)\n",
    "\n",
    "# Save bank config\n",
    "bank_config = {\n",
    "    \"bank_size\": bank_size,\n",
    "    \"mchirp_min\": mchirp_min,\n",
    "    \"mchirp_max\": mchirp_max,\n",
    "    \"q_min\": q_min,\n",
    "    \"f_ref\": f_ref,\n",
    "    \"fbin\": config.DEFAULT_FBIN.tolist(),\n",
    "    \"approximant\": approximant,\n",
    "    \"m_arr\": [2, 1, 3, 4],\n",
    "    \"seed\": seed,\n",
    "}\n",
    "bank_config_path = bank_dir / \"bank_config.json\"\n",
    "with open(bank_config_path, \"w\") as f:\n",
    "    json.dump(bank_config, f, indent=4)\n",
    "\n",
    "print(f\"Saved bank to: {bank_dir}\")\n",
    "print(f\"Bank size: {len(bank_samples):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate waveforms (this may take a few minutes)\n",
    "waveform_dir = bank_dir / \"waveforms\"\n",
    "print(f\"Generating waveforms using {n_pool} cores...\")\n",
    "waveform_banks.create_waveform_bank_from_samples(\n",
    "    samples_path=samples_path,\n",
    "    bank_config_path=bank_config_path,\n",
    "    waveform_dir=waveform_dir,\n",
    "    n_pool=n_pool,\n",
    "    blocksize=blocksize,\n",
    "    approximant=approximant,\n",
    ")\n",
    "print(\"Waveform generation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Select Intrinsic Samples Based on Chirp Mass\n",
    "\n",
    "Select samples from the bank based on chirp mass: `mchirp_guess Â± 8/snr * (mchirp_guess/10)^1.7`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bank and compute chirp masses\n",
    "bank_df = pd.read_feather(samples_path)\n",
    "bank_mchirp = gw_utils.m1m2_to_mchirp(bank_df[\"m1\"], bank_df[\"m2\"])\n",
    "\n",
    "# Chirp mass selection based on guess and SNR\n",
    "mchirp_guess = chirp_mass  # Use injection chirp mass as guess\n",
    "mchirp_window = 2 * (8 / snr) * (mchirp_guess / 10) ** 1.7\n",
    "mchirp_min_select = mchirp_guess - mchirp_window\n",
    "mchirp_max_select = mchirp_guess + mchirp_window\n",
    "\n",
    "# Select indices\n",
    "preselected_indices = np.where(\n",
    "    (bank_mchirp >= mchirp_min_select) & (bank_mchirp <= mchirp_max_select)\n",
    ")[0]\n",
    "\n",
    "print(f\"Chirp mass guess: {mchirp_guess:.2f}\")\n",
    "print(f\"Selection window: [{mchirp_min_select:.2f}, {mchirp_max_select:.2f}]\")\n",
    "print(f\"Selected {len(preselected_indices):,} samples from bank of {len(bank_df):,}\")\n",
    "print(f\"Selection fraction: {len(preselected_indices) / len(bank_df):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Phase 1 & 2 - Incoherent Filtering and Global Cut\n",
    "\n",
    "The new inference structure has 5 phases. Here we demonstrate:\n",
    "- **Phase 1**: Perform incoherent single-detector likelihood evaluations (without threshold)\n",
    "- **Phase 2**: Apply global cut based on maximum likelihood across all banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up inference parameters\n",
    "n_int = len(bank_df) // 4  # Use a quarter just for demonstration\n",
    "n_phi = 32  # Number of phi_ref samples for single-detector evaluation\n",
    "n_t = 128  # Number of time samples\n",
    "single_detector_blocksize = 2048\n",
    "max_incoherent_lnlike_drop = 20\n",
    "\n",
    "# Create posterior to get par_dic_0 (matching inference.run() pattern)\n",
    "fbin = np.array(bank_config[\"fbin\"])\n",
    "f_ref = bank_config[\"f_ref\"]\n",
    "posterior_kwargs = {\n",
    "    \"likelihood_class\": RelativeBinningLikelihood,\n",
    "    \"approximant\": approximant,\n",
    "    \"prior_class\": \"CartesianIASPrior\",\n",
    "}\n",
    "likelihood_kwargs = {\"fbin\": fbin, \"pn_phase_tol\": None}\n",
    "ref_wf_finder_kwargs = {\"time_range\": (-1e-1, +1e-1), \"f_ref\": f_ref}\n",
    "\n",
    "posterior = Posterior.from_event(\n",
    "    event=event_data,\n",
    "    mchirp_guess=mchirp_guess,\n",
    "    likelihood_kwargs=likelihood_kwargs,\n",
    "    ref_wf_finder_kwargs=ref_wf_finder_kwargs,\n",
    "    **posterior_kwargs,\n",
    ")\n",
    "par_dic_0 = posterior.likelihood.par_dic_0.copy()\n",
    "\n",
    "# Phase 1: Collect incoherent likelihoods without threshold\n",
    "print(\"=== Phase 1: Incoherent filtering (per bank) ===\")\n",
    "print(\"Collecting incoherent likelihoods (no threshold applied)...\")\n",
    "inds_unfiltered, lnlike_di, incoherent_lnlikes = (\n",
    "    inference.collect_int_samples_from_single_detectors(\n",
    "        event_data=event_data,\n",
    "        par_dic_0=par_dic_0,\n",
    "        single_detector_blocksize=single_detector_blocksize,\n",
    "        n_int=n_int,\n",
    "        n_phi=n_phi,\n",
    "        n_t=n_t,\n",
    "        bank_folder=bank_dir,\n",
    "        i_int_start=0,\n",
    "        max_incoherent_lnlike_drop=max_incoherent_lnlike_drop,\n",
    "        preselected_indices=preselected_indices,\n",
    "        apply_threshold=False,  # Don't apply threshold yet\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Evaluated {len(inds_unfiltered):,} intrinsic samples\")\n",
    "print(f\"Best incoherent lnL: {incoherent_lnlikes.max():.2f}\")\n",
    "\n",
    "# Phase 2: Apply global cut\n",
    "print(\"\\n=== Phase 2: Global cut ===\")\n",
    "global_max_lnlike = incoherent_lnlikes.max()\n",
    "global_threshold = global_max_lnlike - max_incoherent_lnlike_drop\n",
    "print(f\"Global maximum incoherent lnlike: {global_max_lnlike:.2f}\")\n",
    "print(f\"Global threshold: {global_threshold:.2f}\")\n",
    "\n",
    "selected = incoherent_lnlikes >= global_threshold\n",
    "inds = inds_unfiltered[selected]\n",
    "incoherent_lnlikes_filtered = incoherent_lnlikes[selected]\n",
    "lnlike_di_filtered = lnlike_di[:, selected]\n",
    "\n",
    "num_selected = len(inds)\n",
    "frac_of_bank = num_selected / len(bank_df)\n",
    "frac_of_preselected = num_selected / len(preselected_indices)\n",
    "print(f\"Selected {num_selected:,} intrinsic samples after global cut\")\n",
    "print(f\"Fraction of original bank: {frac_of_bank:.2%}\")\n",
    "print(f\"Fraction of preselected samples: {frac_of_preselected:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the samples selected and the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get selected samples from bank\n",
    "selected_samples = bank_df.iloc[inds].copy()\n",
    "selected_samples[\"mchirp\"] = gw_utils.m1m2_to_mchirp(\n",
    "    selected_samples[\"m1\"], selected_samples[\"m2\"]\n",
    ")\n",
    "selected_samples[\"lnq\"] = np.log(selected_samples[\"m2\"] / selected_samples[\"m1\"])\n",
    "selected_samples[\"chieff\"] = gw_utils.chieff(\n",
    "    *selected_samples[[\"m1\", \"m2\", \"s1z\", \"s2z\"]].values.T\n",
    ")\n",
    "\n",
    "# Add mchirp, lnq, chieff to original bank for plotting\n",
    "bank_df_plot = bank_df.copy()\n",
    "bank_df_plot[\"mchirp\"] = bank_mchirp\n",
    "bank_df_plot[\"lnq\"] = np.log(bank_df_plot[\"m2\"] / bank_df_plot[\"m1\"])\n",
    "bank_df_plot[\"chieff\"] = gw_utils.chieff(\n",
    "    *bank_df_plot[[\"m1\", \"m2\", \"s1z\", \"s2z\"]].values.T\n",
    ")\n",
    "\n",
    "# Create MultiCornerPlot\n",
    "plot_params = [\n",
    "    \"mchirp\",\n",
    "    \"lnq\",\n",
    "    \"chieff\",\n",
    "    \"s1z\",\n",
    "    \"s2z\",\n",
    "    \"s1x_n\",\n",
    "    \"s1y_n\",\n",
    "    \"s2x_n\",\n",
    "    \"s2y_n\",\n",
    "]\n",
    "comparison_plot = gw_plotting.MultiCornerPlot(\n",
    "    [bank_df_plot, selected_samples],\n",
    "    params=plot_params,\n",
    "    smooth=1.0,\n",
    "    labels=[\"Original Bank\", \"Selected Samples\"],\n",
    ")\n",
    "comparison_plot.plot(max_figsize=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Phase 3 - Extrinsic Sample Generation\n",
    "\n",
    "Generate extrinsic samples using the filtered intrinsic samples from Phase 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up rundir for saving intermediate results\n",
    "rundir = ARTIFACTS_DIR / \"run\"\n",
    "rundir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save intrinsic sample indices (filtered from Phase 2)\n",
    "np.savez(\n",
    "    rundir / \"intrinsic_samples.npz\",\n",
    "    inds=inds,\n",
    "    lnlikes_di=lnlike_di_filtered,\n",
    "    incoherent_lnlikes=incoherent_lnlikes_filtered,\n",
    ")\n",
    "\n",
    "# Phase 3: Generate extrinsic samples using filtered intrinsic samples\n",
    "print(\"\\n=== Phase 3: Extrinsic sample generation ===\")\n",
    "n_ext = 2**10  # Number of extrinsic samples\n",
    "seed_ext = 1337\n",
    "\n",
    "from dot_pe.coherent_processing import CoherentExtrinsicSamplesGenerator\n",
    "from dot_pe.marginalization import MarginalizationExtrinsicSamplerFreeLikelihood\n",
    "from cogwheel.waveform import WaveformGenerator\n",
    "\n",
    "wfg = WaveformGenerator.from_event_data(event_data, approximant)\n",
    "fbin = np.array(bank_config[\"fbin\"])\n",
    "\n",
    "marg_ext_like = MarginalizationExtrinsicSamplerFreeLikelihood(\n",
    "    event_data, wfg, par_dic_0, fbin, coherent_score={\"min_n_effective_prior\": 100}\n",
    ")\n",
    "\n",
    "# Note: intrinsic_bank_file and waveform_dir are not used in multibank\n",
    "# get_marg_info_multibank; choosing first_bank_path is only for compatibility\n",
    "ext_sample_generator = CoherentExtrinsicSamplesGenerator(\n",
    "    likelihood=marg_ext_like,\n",
    "    intrinsic_bank_file=samples_path,\n",
    "    waveform_dir=waveform_dir,\n",
    "    seed=seed_ext,\n",
    ")\n",
    "\n",
    "# Use filtered indices from Phase 2\n",
    "get_marg_info_kwargs = {\n",
    "    \"save_marg_info\": True,\n",
    "    \"save_marg_info_dir\": rundir,\n",
    "    \"n_combine\": 16,\n",
    "    \"indices\": inds,  # Use filtered indices from Phase 2\n",
    "    \"single_marg_info_min_n_effective_prior\": 32,\n",
    "}\n",
    "\n",
    "print(\"Drawing extrinsic samples from filtered intrinsic samples...\")\n",
    "extrinsic_samples, response_dpe, timeshift_dbe = (\n",
    "    ext_sample_generator.draw_extrinsic_samples_from_indices(\n",
    "        n_ext, get_marg_info_kwargs=get_marg_info_kwargs\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save extrinsic samples\n",
    "extrinsic_samples.to_feather(rundir / \"extrinsic_samples.feather\")\n",
    "np.save(arr=response_dpe, file=rundir / \"response_dpe.npy\")\n",
    "np.save(arr=timeshift_dbe, file=rundir / \"timeshift_dbe.npy\")\n",
    "\n",
    "print(f\"Generated {len(extrinsic_samples):,} extrinsic samples\")\n",
    "print(f\"Saved to: {rundir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot extrinsic parameters\n",
    "ext_params = [\"lat\", \"lon\", \"psi\", \"t_geocenter\"]\n",
    "extrinsic_samples_unweighted = extrinsic_samples.copy()\n",
    "extrinsic_samples_unweighted[\"weights\"] = 1 / len(extrinsic_samples_unweighted)\n",
    "ext_plot = gw_plotting.MultiCornerPlot(\n",
    "    [\n",
    "        extrinsic_samples_unweighted,\n",
    "        extrinsic_samples,\n",
    "    ],\n",
    "    params=ext_params,\n",
    "    smooth=1.0,\n",
    "    weights_col=\"weights\",\n",
    "    labels=[\n",
    "        \"Unweighted Samples\",\n",
    "        \"Weighted samples\",\n",
    "    ],\n",
    ")\n",
    "ext_plot.plot(max_figsize=4.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Phase 4 - Coherent Likelihood Evaluations\n",
    "\n",
    "Perform coherent multi-detector likelihood evaluations using filtered intrinsic samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Run coherent inference using filtered intrinsic samples\n",
    "print(\"\\n=== Phase 4: Coherent inference (per bank) ===\")\n",
    "n_phi_coherent = 32  # Number of phi_ref samples for coherent evaluation\n",
    "blocksize = 1024  # max number of intrinsic and extrinsic samples to evaluate simultaneously\n",
    "m_arr = np.array(bank_config[\"m_arr\"])\n",
    "\n",
    "print(\"Running coherent likelihood evaluations...\")\n",
    "(\n",
    "    ln_evidence,\n",
    "    ln_evidence_discarded,\n",
    "    n_effective,\n",
    "    n_effective_i,\n",
    "    n_effective_e,\n",
    "    n_distance_marginalizations,\n",
    ") = inference.run_coherent_inference(\n",
    "    event_data=event_data,\n",
    "    bank_rundir=rundir,\n",
    "    top_rundir=rundir,\n",
    "    par_dic_0=par_dic_0,\n",
    "    bank_folder=bank_dir,\n",
    "    n_total_samples=n_phi_coherent * n_ext * n_int,  # Total MC samples attempted\n",
    "    inds=inds,  # Use filtered indices from Phase 2\n",
    "    n_ext=n_ext,\n",
    "    n_phi=n_phi_coherent,\n",
    "    m_arr=m_arr,\n",
    "    blocksize=blocksize,\n",
    "    size_limit=10**6,\n",
    "    max_bestfit_lnlike_diff=20,\n",
    ")\n",
    "\n",
    "print(\"Coherent inference complete!\")\n",
    "print(f\"  ln_evidence: {ln_evidence:.2f}\")\n",
    "print(f\"  n_effective: {n_effective:.2f}\")\n",
    "print(f\"  n_effective_i: {n_effective_i:.2f}\")\n",
    "print(f\"  n_effective_e: {n_effective_e:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Phase 5 - Post-processing and Standardization\n",
    "\n",
    "Standardize samples (mimics what inference.run() does via postprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Post-processing and standardization\n",
    "print(\"\\n=== Phase 5: Post-processing and standardization ===\")\n",
    "samples = inference.postprocess(\n",
    "    event_data=event_data,\n",
    "    rundir=rundir,\n",
    "    bank_folder=bank_dir,\n",
    "    n_phi=n_phi_coherent,\n",
    "    pr=posterior.prior,\n",
    "    draw_subset=n_effective * 2,\n",
    ")\n",
    "\n",
    "# Save standardized samples\n",
    "samples_path_final = rundir / \"samples.feather\"\n",
    "samples.to_feather(samples_path_final)\n",
    "print(f\"Standardized samples saved to: {samples_path_final}\")\n",
    "print(f\"Samples shape: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corner plot with true injection parameters\n",
    "\n",
    "# Compute true values for plotting\n",
    "true_mchirp = gw_utils.m1m2_to_mchirp(injection_par_dic[\"m1\"], injection_par_dic[\"m2\"])\n",
    "true_lnq = np.log(injection_par_dic[\"m2\"] / injection_par_dic[\"m1\"])\n",
    "true_chieff = gw_utils.chieff(\n",
    "    injection_par_dic[\"m1\"],\n",
    "    injection_par_dic[\"m2\"],\n",
    "    injection_par_dic[\"s1z\"],\n",
    "    injection_par_dic[\"s2z\"],\n",
    ")\n",
    "\n",
    "true_values = injection_par_dic | {\n",
    "    \"mchirp\": true_mchirp,\n",
    "    \"lnq\": true_lnq,\n",
    "    \"chieff\": true_chieff,\n",
    "}\n",
    "\n",
    "# Load standardized samples (created in previous cell)\n",
    "samples = pd.read_feather(samples_path_final)\n",
    "\n",
    "# Create corner plot\n",
    "\n",
    "params = [\n",
    "    \"mchirp\",\n",
    "    \"lnq\",\n",
    "    \"chieff\",\n",
    "    \"iota\",\n",
    "    \"ra\",\n",
    "    \"dec\",\n",
    "    \"d_luminosity\",\n",
    "    \"bestfit_lnlike\",\n",
    "]\n",
    "corner_plot = gw_plotting.CornerPlot(\n",
    "    samples,\n",
    "    params=params,\n",
    "    smooth=1.0,\n",
    ")\n",
    "corner_plot.plot(max_figsize=7)\n",
    "\n",
    "# Add true injection parameters\n",
    "corner_plot.scatter_points(\n",
    "    true_values, colors=\"red\", marker=\".\", s=200, label=\"Injection\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Streamlined Process Using inference.run()\n",
    "\n",
    "Finally, here's how to use `inference.run()` to do everything in one streamlined call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new rundir for streamlined run\n",
    "streamlined_rundir = ARTIFACTS_DIR / \"run_streamlined\"\n",
    "streamlined_rundir.mkdir(parents=True, exist_ok=True)\n",
    "bank_df = pd.read_feather(\"artifacts/bank/intrinsic_sample_bank.feather\")\n",
    "n_int = len(bank_df)\n",
    "n_ext = 1024\n",
    "n_phi = 50\n",
    "n_t = 128\n",
    "seed_ext = sum([int(str(i) * i) for i in range(1, 10)])\n",
    "blocksize = 2048\n",
    "single_detector_blocksize = 2048\n",
    "mchirp_guess = chirp_mass\n",
    "# Run streamlined inference\n",
    "print(\"Running streamlined inference with inference.run()...\")\n",
    "final_rundir = inference.run(\n",
    "    event=event_data,\n",
    "    bank_folder=bank_dir,\n",
    "    n_int=n_int,\n",
    "    n_ext=n_ext,\n",
    "    n_phi=n_phi,\n",
    "    n_t=n_t,\n",
    "    blocksize=blocksize,\n",
    "    single_detector_blocksize=single_detector_blocksize,\n",
    "    seed=seed_ext,\n",
    "    event_dir=str(streamlined_rundir),\n",
    "    mchirp_guess=mchirp_guess,\n",
    "    preselected_indices=None,\n",
    "    max_incoherent_lnlike_drop=20,\n",
    "    max_bestfit_lnlike_diff=20,\n",
    "    draw_subset=True,\n",
    ")\n",
    "\n",
    "print(\"\\nStreamlined inference complete!\")\n",
    "print(f\"Results saved to: {final_rundir}\")\n",
    "\n",
    "# Load summary\n",
    "summary_path = final_rundir / \"summary_results.json\"\n",
    "summary = utils.read_json(summary_path)\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  n_effective: {summary['n_effective']:.2f}\")\n",
    "print(f\"  n_effective_i: {summary['n_effective_i']:.2f}\")\n",
    "print(f\"  n_effective_e: {summary['n_effective_e']:.2f}\")\n",
    "print(f\"  ln_evidence: {summary['ln_evidence']:.2f}\")\n",
    "\n",
    "# Load final samples\n",
    "final_samples = pd.read_feather(final_rundir / \"samples.feather\")\n",
    "print(f\"\\nFinal samples shape: {final_samples.shape}\")\n",
    "print(f\"Final samples saved to: {final_rundir / 'samples.feather'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corner plot with true injection parameters\n",
    "\n",
    "# Compute true values for plotting\n",
    "true_mchirp = gw_utils.m1m2_to_mchirp(injection_par_dic[\"m1\"], injection_par_dic[\"m2\"])\n",
    "true_lnq = np.log(injection_par_dic[\"m2\"] / injection_par_dic[\"m1\"])\n",
    "true_chieff = gw_utils.chieff(\n",
    "    injection_par_dic[\"m1\"],\n",
    "    injection_par_dic[\"m2\"],\n",
    "    injection_par_dic[\"s1z\"],\n",
    "    injection_par_dic[\"s2z\"],\n",
    ")\n",
    "\n",
    "true_values = injection_par_dic | {\n",
    "    \"mchirp\": true_mchirp,\n",
    "    \"lnq\": true_lnq,\n",
    "    \"chieff\": true_chieff,\n",
    "}\n",
    "\n",
    "# Load standardized samples (created in previous cell)\n",
    "samples = pd.read_feather(ARTIFACTS_DIR / \"run_streamlined/run_0/samples.feather\")\n",
    "\n",
    "# Create corner plot\n",
    "\n",
    "params = [\n",
    "    \"mchirp\",\n",
    "    \"lnq\",\n",
    "    \"chieff\",\n",
    "    \"iota\",\n",
    "    \"ra\",\n",
    "    \"dec\",\n",
    "    \"d_luminosity\",\n",
    "    \"bestfit_lnlike\",\n",
    "]\n",
    "corner_plot = gw_plotting.CornerPlot(\n",
    "    samples,\n",
    "    params=params,\n",
    "    smooth=1.0,\n",
    ")\n",
    "corner_plot.plot(max_figsize=7)\n",
    "\n",
    "# Add true injection parameters\n",
    "corner_plot.scatter_points(\n",
    "    true_values, colors=\"red\", marker=\".\", s=200, label=\"Injection\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Streamlined with multiple banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogwheel.gw_utils import m1m2_to_mchirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two banks\n",
    "bank_names = [\"bank_15_20\", \"bank_20_30\"]\n",
    "mass_ranges = [(15, 20), (20, 30)]\n",
    "for i, (bank_name, mass_range) in enumerate(zip(bank_names, mass_ranges)):\n",
    "    bank_dir = ARTIFACTS_DIR / bank_name\n",
    "    bank_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    bank_size = 2**15\n",
    "    mchirp_min = mass_range[0]\n",
    "    mchirp_max = mass_range[1]\n",
    "    q_min = 0.2\n",
    "    f_ref = 50.0\n",
    "    seed = 888 + i\n",
    "    n_pool = 4\n",
    "    blocksize = 4096\n",
    "    approximant = \"IMRPhenomXPHM\"\n",
    "\n",
    "    # Generate bank samples\n",
    "    powerlaw_prior = PowerLawIntrinsicIASPrior(\n",
    "        mchirp_range=(mchirp_min, mchirp_max),\n",
    "        q_min=q_min,\n",
    "        f_ref=f_ref,\n",
    "    )\n",
    "\n",
    "    print(f\"Generating {bank_size:,} bank samples...\")\n",
    "    bank_samples = powerlaw_prior.generate_random_samples(\n",
    "        bank_size, seed=seed, return_lnz=False\n",
    "    )\n",
    "\n",
    "    # Compute derived quantities and weights\n",
    "    bank_samples[\"mchirp\"] = gw_utils.m1m2_to_mchirp(\n",
    "        bank_samples[\"m1\"], bank_samples[\"m2\"]\n",
    "    )\n",
    "    bank_samples[\"lnq\"] = np.log(bank_samples[\"m2\"] / bank_samples[\"m1\"])\n",
    "    bank_samples[\"chieff\"] = gw_utils.chieff(\n",
    "        *bank_samples[[\"m1\", \"m2\", \"s1z\", \"s2z\"]].values.T\n",
    "    )\n",
    "    mchirp_values = bank_samples[\"mchirp\"].values\n",
    "    bank_samples[\"log_prior_weights\"] = 1.7 * np.log(mchirp_values)\n",
    "\n",
    "    # Save bank\n",
    "    bank_columns = [\n",
    "        \"m1\",\n",
    "        \"m2\",\n",
    "        \"s1z\",\n",
    "        \"s1x_n\",\n",
    "        \"s1y_n\",\n",
    "        \"s2z\",\n",
    "        \"s2x_n\",\n",
    "        \"s2y_n\",\n",
    "        \"iota\",\n",
    "        \"log_prior_weights\",\n",
    "    ]\n",
    "    samples_path = bank_dir / \"intrinsic_sample_bank.feather\"\n",
    "    bank_samples[bank_columns].to_feather(samples_path)\n",
    "\n",
    "    # Save bank config\n",
    "    bank_config = {\n",
    "        \"bank_size\": bank_size,\n",
    "        \"mchirp_min\": mchirp_min,\n",
    "        \"mchirp_max\": mchirp_max,\n",
    "        \"q_min\": q_min,\n",
    "        \"f_ref\": f_ref,\n",
    "        \"fbin\": config.DEFAULT_FBIN.tolist(),\n",
    "        \"approximant\": approximant,\n",
    "        \"m_arr\": [2, 1, 3, 4],\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    bank_config_path = bank_dir / \"bank_config.json\"\n",
    "    with open(bank_config_path, \"w\") as f:\n",
    "        json.dump(bank_config, f, indent=4)\n",
    "\n",
    "    print(f\"Saved bank to: {bank_dir}\")\n",
    "    print(f\"Bank size: {len(bank_samples):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerlaw_prior = PowerLawIntrinsicIASPrior(\n",
    "    mchirp_range=(15, 30),\n",
    "    q_min=q_min,\n",
    "    f_ref=f_ref,\n",
    ")\n",
    "\n",
    "x = powerlaw_prior.subpriors[1].generate_random_samples(10**6)\n",
    "powerlaw_prior.subpriors[1].transform_samples(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"mchirp\"] = m1m2_to_mchirp(**x[[\"m1\", \"m2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_low_mass = len(x.query(\"mchirp <=20\"))\n",
    "n_high_mass = len(x.query(\"mchirp>20\"))\n",
    "\n",
    "low_mass_frac = n_low_mass / (n_high_mass + n_low_mass)\n",
    "high_mass_frac = n_high_mass / (n_high_mass + n_low_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_mass_frac, high_mass_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new rundir for streamlined run\n",
    "streamlined_rundir = ARTIFACTS_DIR / \"run_multibank\"\n",
    "streamlined_rundir.mkdir(parents=True, exist_ok=True)\n",
    "n_int = 2**15\n",
    "n_ext = 1024\n",
    "n_phi = 50\n",
    "blocksize = 2048\n",
    "single_detector_blocksize = 2048\n",
    "bank_folders = [ARTIFACTS_DIR / b for b in bank_names]\n",
    "# Run streamlined inference\n",
    "print(\"Running streamlined inference with inference.run()...\")\n",
    "final_rundir = inference.run(\n",
    "    event=event_data,\n",
    "    bank_folder=bank_folders,\n",
    "    n_int=n_int,\n",
    "    n_ext=n_ext,\n",
    "    n_phi=n_phi_coherent,\n",
    "    n_t=n_t,\n",
    "    blocksize=blocksize,\n",
    "    single_detector_blocksize=single_detector_blocksize,\n",
    "    seed=seed_ext,\n",
    "    event_dir=str(streamlined_rundir),\n",
    "    mchirp_guess=mchirp_guess,\n",
    "    preselected_indices=None,\n",
    "    max_incoherent_lnlike_drop=20,\n",
    "    max_bestfit_lnlike_diff=20,\n",
    "    draw_subset=True,\n",
    ")\n",
    "\n",
    "print(\"\\nStreamlined inference complete!\")\n",
    "print(f\"Results saved to: {final_rundir}\")\n",
    "\n",
    "# Load summary\n",
    "summary_path = final_rundir / \"summary_results.json\"\n",
    "summary = utils.read_json(summary_path)\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  n_effective: {summary['n_effective']:.2f}\")\n",
    "print(f\"  n_effective_i: {summary['n_effective_i']:.2f}\")\n",
    "print(f\"  n_effective_e: {summary['n_effective_e']:.2f}\")\n",
    "print(f\"  ln_evidence: {summary['ln_evidence']:.2f}\")\n",
    "\n",
    "# Load final samples\n",
    "final_samples = pd.read_feather(final_rundir / \"samples.feather\")\n",
    "print(f\"\\nFinal samples shape: {final_samples.shape}\")\n",
    "print(f\"Final samples saved to: {final_rundir / 'samples.feather'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs = [ARTIFACTS_DIR / b for b in bank_names]\n",
    "[bf.exists() for bf in bfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dot-pe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
