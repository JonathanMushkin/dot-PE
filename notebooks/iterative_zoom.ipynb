{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from cogwheel import data, gw_utils, gw_plotting, utils\n",
    "from dot_pe import inference, waveform_banks, config\n",
    "from dot_pe.zoom import zoom_iteration, zoom\n",
    "from dot_pe.utils import load_intrinsic_samples_from_rundir\n",
    "from dot_pe.power_law_mass_prior import PowerLawIntrinsicIASPrior\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "\n",
    "# Set up artifacts directory for all outputs\n",
    "ARTIFACTS_DIR = Path(\"./artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_OUTPUT_ROOT = ARTIFACTS_DIR / \"iterative_zoom\"\n",
    "NOTEBOOK_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom Iteration Demo\n",
    "\n",
    "This notebook demonstrates the zoom iteration workflow: creating an injection, building an initial bank, running inference, and performing zoom iterations to improve sampling efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Injection\n",
    "\n",
    "Create a high SNR injection with low luminosity distance for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Injection setup\n",
    "injection_event_name = \"zoom_example_event\"\n",
    "\n",
    "injection_dir = NOTEBOOK_OUTPUT_ROOT / \"injection\"\n",
    "injection_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "injection_event_path = injection_dir / f\"{injection_event_name}.npz\"\n",
    "\n",
    "injection_event_kwargs = dict(\n",
    "    eventname=injection_event_name,\n",
    "    detector_names=\"HL\",\n",
    "    duration=120.0,\n",
    "    asd_funcs=[\"asd_H_O3\", \"asd_L_O3\"],\n",
    "    tgps=0.0,\n",
    "    fmax=1600.0,\n",
    "    seed=20250311,\n",
    ")\n",
    "\n",
    "injection_mchirp = 30.0\n",
    "injection_mass_ratio = 0.5\n",
    "injection_m1, injection_m2 = gw_utils.mchirpeta_to_m1m2(\n",
    "    injection_mchirp, gw_utils.q_to_eta(injection_mass_ratio)\n",
    ")\n",
    "\n",
    "injection_parameters = dict(\n",
    "    m1=injection_m1,\n",
    "    m2=injection_m2,\n",
    "    ra=0.5,\n",
    "    dec=0.5,\n",
    "    iota=np.pi / 3,\n",
    "    psi=1.0,\n",
    "    phi_ref=12.0,\n",
    "    s1z=0.6,\n",
    "    s2z=0.6,\n",
    "    s1x_n=0.1,\n",
    "    s1y_n=0.2,\n",
    "    s2x_n=0.3,\n",
    "    s2y_n=-0.2,\n",
    "    l1=0.0,\n",
    "    l2=0.0,\n",
    "    tgps=0.0,\n",
    "    f_ref=50.0,\n",
    "    d_luminosity=1.5e3,\n",
    "    t_geocenter=0.0,\n",
    ")\n",
    "\n",
    "injection_waveform_model = \"IMRPhenomXPHM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save injection\n",
    "injection_event_data = data.EventData.gaussian_noise(**injection_event_kwargs)\n",
    "injection_event_data.inject_signal(injection_parameters, injection_waveform_model)\n",
    "\n",
    "injection_event_data.to_npz(filename=injection_event_path, overwrite=True)\n",
    "print(f\"Injection saved to {injection_event_path}\")\n",
    "\n",
    "injection_log_likelihood = (\n",
    "    injection_event_data.injection[\"d_h\"]\n",
    "    - 0.5 * injection_event_data.injection[\"h_h\"]\n",
    ").sum()\n",
    "print(f\"Injection SNR^2 = {2 * injection_log_likelihood:.3g}\")\n",
    "print(f\"Injection SNR = {np.sqrt(2 * injection_log_likelihood):.3g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Initial Bank\n",
    "\n",
    "Create a wide prior bank with 2^16 samples for initial inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial bank setup\n",
    "initial_bank_dir = NOTEBOOK_OUTPUT_ROOT / \"bank\"\n",
    "initial_bank_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "initial_waveform_dir = initial_bank_dir / \"waveforms\"\n",
    "initial_waveform_dir.mkdir(exist_ok=True)\n",
    "\n",
    "initial_bank_size = 2**16\n",
    "initial_mchirp_min = 15.0\n",
    "initial_mchirp_max = 65.0\n",
    "initial_q_min = 0.2\n",
    "initial_f_ref = 50.0\n",
    "initial_fbin = config.DEFAULT_FBIN\n",
    "initial_approximant = \"IMRPhenomXPHM\"\n",
    "initial_bank_seed = 1701\n",
    "\n",
    "initial_bank_samples_path = initial_bank_dir / \"intrinsic_sample_bank.feather\"\n",
    "initial_bank_config_path = initial_bank_dir / \"bank_config.json\"\n",
    "\n",
    "initial_powerlaw_prior_kwargs = dict(\n",
    "    mchirp_range=(initial_mchirp_min, initial_mchirp_max),\n",
    "    q_min=initial_q_min,\n",
    "    f_ref=initial_f_ref,\n",
    ")\n",
    "\n",
    "initial_bank_columns = [\n",
    "    \"m1\",\n",
    "    \"m2\",\n",
    "    \"s1z\",\n",
    "    \"s1x_n\",\n",
    "    \"s1y_n\",\n",
    "    \"s2z\",\n",
    "    \"s2x_n\",\n",
    "    \"s2y_n\",\n",
    "    \"iota\",\n",
    "    \"log_prior_weights\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build initial bank\n",
    "print(\"Generating samples for initial bank...\")\n",
    "initial_powerlaw_prior = PowerLawIntrinsicIASPrior(**initial_powerlaw_prior_kwargs)\n",
    "\n",
    "initial_powerlaw_samples = initial_powerlaw_prior.generate_random_samples(\n",
    "    initial_bank_size, seed=initial_bank_seed, return_lnz=False\n",
    ")\n",
    "print(f\"Generated {len(initial_powerlaw_samples):,} samples\")\n",
    "\n",
    "initial_powerlaw_samples[\"mchirp\"] = gw_utils.m1m2_to_mchirp(\n",
    "    initial_powerlaw_samples[\"m1\"], initial_powerlaw_samples[\"m2\"]\n",
    ")\n",
    "initial_powerlaw_samples[\"lnq\"] = np.log(\n",
    "    initial_powerlaw_samples[\"m2\"] / initial_powerlaw_samples[\"m1\"]\n",
    ")\n",
    "initial_powerlaw_samples[\"chieff\"] = gw_utils.chieff(\n",
    "    *initial_powerlaw_samples[[\"m1\", \"m2\", \"s1z\", \"s2z\"]].values.T\n",
    ")\n",
    "\n",
    "initial_bank_mchirp_values = initial_powerlaw_samples[\"mchirp\"].values\n",
    "initial_powerlaw_samples[\"log_prior_weights\"] = 1.7 * np.log(initial_bank_mchirp_values)\n",
    "\n",
    "initial_powerlaw_samples[initial_bank_columns].to_feather(initial_bank_samples_path)\n",
    "print(f\"Saved samples to {initial_bank_samples_path}\")\n",
    "\n",
    "initial_bank_config = {\n",
    "    \"bank_size\": initial_bank_size,\n",
    "    \"mchirp_min\": initial_mchirp_min,\n",
    "    \"mchirp_max\": initial_mchirp_max,\n",
    "    \"q_min\": initial_q_min,\n",
    "    \"f_ref\": initial_f_ref,\n",
    "    \"fbin\": initial_fbin.tolist(),\n",
    "    \"approximant\": initial_approximant,\n",
    "    \"seed\": initial_bank_seed,\n",
    "}\n",
    "\n",
    "with open(initial_bank_config_path, \"w\") as fp:\n",
    "    json.dump(obj=initial_bank_config, fp=fp, indent=4)\n",
    "print(f\"Saved bank config to {initial_bank_config_path}\")\n",
    "\n",
    "print(\"Generating waveforms for initial bank...\")\n",
    "waveform_banks.create_waveform_bank_from_samples(\n",
    "    samples_path=initial_bank_samples_path,\n",
    "    bank_config_path=initial_bank_config_path,\n",
    "    waveform_dir=initial_waveform_dir,\n",
    "    n_pool=4,\n",
    "    blocksize=4096,\n",
    "    approximant=initial_approximant,\n",
    ")\n",
    "print(f\"Initial bank created at {initial_bank_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Initial Inference\n",
    "\n",
    "Run inference with the initial bank and display summary results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial inference setup\n",
    "initial_inference_event_data = data.EventData.from_npz(filename=injection_event_path)\n",
    "\n",
    "initial_inference_dir = (\n",
    "    NOTEBOOK_OUTPUT_ROOT / \"inference_runs\" / initial_inference_event_data.eventname\n",
    ")\n",
    "initial_inference_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "initial_inference_kwargs = dict(\n",
    "    event_dir=str(initial_inference_dir),\n",
    "    event=initial_inference_event_data,\n",
    "    bank_folder=str(initial_bank_dir),\n",
    "    n_int=initial_bank_size,\n",
    "    n_ext=2048,\n",
    "    n_phi=100,\n",
    "    n_t=128,\n",
    "    blocksize=1024,\n",
    "    single_detector_blocksize=1024,\n",
    "    seed=1337,\n",
    "    size_limit=10**6,\n",
    "    draw_subset=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run initial inference\n",
    "initial_rundir = inference.run(**initial_inference_kwargs)\n",
    "print(f\"Inference complete. Results in {initial_rundir}\")\n",
    "\n",
    "initial_summary_path = Path(initial_rundir) / \"summary_results.json\"\n",
    "initial_run_summary = utils.read_json(initial_summary_path)\n",
    "print(\"\\nSummary results:\")\n",
    "print(f\"  n_effective: {initial_run_summary['n_effective']:.2f}\")\n",
    "print(f\"  n_effective_i: {initial_run_summary['n_effective_i']:.2f}\")\n",
    "print(f\"  n_effective_e: {initial_run_summary['n_effective_e']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: First Zoom Iteration\n",
    "\n",
    "Fit a Gaussian zoomer to the top-weighted samples, create a focused bank, and run inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intrinsic samples from initial inference\n",
    "iter1_post_samples = load_intrinsic_samples_from_rundir(initial_rundir)\n",
    "iter1_sample_weights = iter1_post_samples[\"weights\"].values\n",
    "\n",
    "# Select top weight samples for zoomer fit\n",
    "iter1_weight_threshold = 0.9\n",
    "iter1_sorted_idx = np.argsort(iter1_sample_weights)[::-1]\n",
    "iter1_cumsum_weights = np.cumsum(iter1_sample_weights[iter1_sorted_idx])\n",
    "iter1_n_selected = max(\n",
    "    np.searchsorted(iter1_cumsum_weights, iter1_weight_threshold) + 1, 20\n",
    ")\n",
    "iter1_selected_idx = iter1_sorted_idx[:iter1_n_selected]\n",
    "iter1_top_samples = iter1_post_samples.iloc[iter1_selected_idx]\n",
    "iter1_top_weights = iter1_sample_weights[iter1_selected_idx]\n",
    "\n",
    "print(\n",
    "    f\"Selected {iter1_n_selected} samples (sum of weights: {iter1_top_weights.sum():.3f})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit zoomer with n_sig=1\n",
    "iter1_prior_kwargs = {\n",
    "    \"mchirp_range\": (initial_mchirp_min, initial_mchirp_max),\n",
    "    \"q_min\": initial_q_min,\n",
    "    \"f_ref\": initial_f_ref,\n",
    "}\n",
    "iter1_zoomer, iter1_cond_sampler, iter1_bounds = zoom_iteration.fit_zoomer(\n",
    "    iter1_top_samples, iter1_top_weights, iter1_prior_kwargs, seed=9731, n_sig=1\n",
    ")\n",
    "\n",
    "print(f\"Zoomer mean: {iter1_zoomer.mean}\")\n",
    "print(\"Zoomer covariance matrix:\")\n",
    "print(iter1_zoomer.cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corner plot with Gaussian overlay\n",
    "iter1_corner_plot = gw_plotting.CornerPlot(\n",
    "    iter1_post_samples, params=[\"mchirp\", \"lnq\", \"chieff\"]\n",
    ")\n",
    "iter1_corner_plot.plot()\n",
    "\n",
    "# Overlay 1D Gaussians on diagonal axes\n",
    "iter1_corner_params = [\"mchirp\", \"lnq\", \"chieff\"]\n",
    "for i, _ in enumerate(iter1_corner_params):\n",
    "    ax = iter1_corner_plot.axes[i, i]\n",
    "    mean_1d = iter1_zoomer.mean[i]\n",
    "    std_1d = np.sqrt(iter1_zoomer.cov[i, i])\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    x = np.linspace(xlim[0], xlim[1], 200)\n",
    "\n",
    "    rv_1d = norm(mean_1d, std_1d)\n",
    "    pdf_1d = rv_1d.pdf(x)\n",
    "\n",
    "    ylim = ax.get_ylim()\n",
    "    pdf_1d_scaled = pdf_1d * (ylim[1] - ylim[0]) / pdf_1d.max() * 0.8\n",
    "\n",
    "    ax.plot(x, pdf_1d_scaled + ylim[0], \"r-\", linewidth=2, label=\"Fitted Gaussian\")\n",
    "    ax.axvline(mean_1d, color=\"red\", linestyle=\"--\", linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Overlay 2D Gaussian contours and mean on off-diagonal axes\n",
    "iter1_corner_pairs = [\n",
    "    ((\"mchirp\", \"lnq\"), (0, 1)),\n",
    "    ((\"mchirp\", \"chieff\"), (0, 2)),\n",
    "    ((\"lnq\", \"chieff\"), (1, 2)),\n",
    "]\n",
    "\n",
    "for (_, _), (i1, i2) in iter1_corner_pairs:\n",
    "    ax = iter1_corner_plot.axes[i2, i1]\n",
    "    mean_2d = iter1_zoomer.mean[[i1, i2]]\n",
    "    cov_2d = iter1_zoomer.cov[[i1, i2], :][:, [i1, i2]]\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    x = np.linspace(xlim[0], xlim[1], 200)\n",
    "    y = np.linspace(ylim[0], ylim[1], 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    pos = np.dstack((X, Y))\n",
    "\n",
    "    rv = multivariate_normal(mean_2d, cov_2d)\n",
    "    Z = rv.pdf(pos)\n",
    "\n",
    "    levels = [rv.pdf(mean_2d) * np.exp(-0.5 * v**2) for v in [2, 1]]\n",
    "\n",
    "    ax.contour(\n",
    "        X, Y, Z, levels=[levels[0]], colors=[\"blue\"], linestyles=[\"--\"], linewidths=1.5\n",
    "    )\n",
    "    ax.contour(\n",
    "        X, Y, Z, levels=[levels[1]], colors=[\"green\"], linestyles=[\"-\"], linewidths=1.5\n",
    "    )\n",
    "    ax.plot(\n",
    "        mean_2d[0], mean_2d[1], \"ro\", markersize=8, markeredgecolor=\"black\", markeredgewidth=1\n",
    "    )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom iteration 1 bank setup\n",
    "iter1_zoom_dir = NOTEBOOK_OUTPUT_ROOT / \"zoom\" / \"iter1\"\n",
    "iter1_zoom_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "iter1_bank_dir = iter1_zoom_dir / \"bank\"\n",
    "iter1_bank_dir.mkdir(parents=True, exist_ok=True)\n",
    "iter1_bank_samples_path = iter1_bank_dir / \"intrinsic_sample_bank.feather\"\n",
    "iter1_bank_config_path = iter1_bank_dir / \"bank_config.json\"\n",
    "iter1_bank_waveform_dir = iter1_bank_dir / \"waveforms\"\n",
    "iter1_bank_waveform_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "iter1_zoom_bank_size = 4096\n",
    "iter1_zoom_bank_seed = 3141\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new bank using zoomer (small bank of 4096 samples)\n",
    "iter1_zoom_bank = zoom_iteration.draw_from_zoomer(\n",
    "    iter1_zoomer,\n",
    "    iter1_cond_sampler,\n",
    "    iter1_bounds,\n",
    "    n_samples=iter1_zoom_bank_size,\n",
    "    seed=iter1_zoom_bank_seed,\n",
    ")\n",
    "\n",
    "iter1_zoom_bank.to_feather(iter1_bank_samples_path)\n",
    "\n",
    "iter1_bank_config = utils.read_json(initial_bank_config_path)\n",
    "iter1_bank_config.update({\"bank_size\": len(iter1_zoom_bank)})\n",
    "with open(iter1_bank_config_path, \"w\") as fp:\n",
    "    json.dump(obj=iter1_bank_config, fp=fp, indent=4)\n",
    "\n",
    "print(f\"Created bank at {iter1_bank_samples_path} ({len(iter1_zoom_bank)} samples)\")\n",
    "\n",
    "waveform_banks.create_waveform_bank_from_samples(\n",
    "    samples_path=iter1_bank_samples_path,\n",
    "    bank_config_path=iter1_bank_config_path,\n",
    "    waveform_dir=iter1_bank_waveform_dir,\n",
    "    n_pool=4,\n",
    "    blocksize=4096,\n",
    "    approximant=initial_approximant,\n",
    ")\n",
    "print(\"Waveforms generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom iteration 1 inference setup\n",
    "iter1_extrinsic_samples_path = Path(initial_rundir) / \"extrinsic_samples.feather\"\n",
    "\n",
    "iter1_inference_kwargs = dict(\n",
    "    event_dir=str(iter1_zoom_dir),\n",
    "    event=initial_inference_event_data,\n",
    "    bank_folder=str(iter1_bank_dir),\n",
    "    n_int=len(iter1_zoom_bank),\n",
    "    n_ext=2048,\n",
    "    n_phi=100,\n",
    "    n_t=128,\n",
    "    blocksize=1024,\n",
    "    single_detector_blocksize=1024,\n",
    "    seed=1618,\n",
    "    size_limit=10**6,\n",
    "    draw_subset=True,\n",
    "    n_draws=1,\n",
    "    extrinsic_samples=str(iter1_extrinsic_samples_path),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run zoom iteration 1 inference\n",
    "iter1_rundir = inference.run(**iter1_inference_kwargs)\n",
    "print(f\"Zoom iteration 1 inference complete. Results in {iter1_rundir}\")\n",
    "\n",
    "iter1_summary_path = Path(iter1_rundir) / \"summary_results.json\"\n",
    "iter1_run_summary = utils.read_json(iter1_summary_path)\n",
    "print(\"\\nZoom iteration 1 results:\")\n",
    "print(f\"  n_effective: {iter1_run_summary['n_effective']:.2f}\")\n",
    "print(f\"  n_effective_i: {iter1_run_summary['n_effective_i']:.2f}\")\n",
    "print(f\"  n_effective_e: {iter1_run_summary['n_effective_e']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Efficiency Calculation\n",
    "\n",
    "Calculate efficiency and extrapolate to 2^16 bank size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate efficiency\n",
    "iter1_effective_intrinsic = iter1_run_summary[\"n_effective_i\"]\n",
    "iter1_bank_size = len(iter1_zoom_bank)\n",
    "iter1_efficiency = iter1_effective_intrinsic / iter1_bank_size\n",
    "\n",
    "print(f\"Zoom iteration 1 efficiency: {iter1_efficiency:.4f}\")\n",
    "print(f\"  n_effective_i: {iter1_effective_intrinsic:.2f}\")\n",
    "print(f\"  bank_size: {iter1_bank_size}\")\n",
    "\n",
    "# Extrapolate to 2^16\n",
    "iter1_extrapolated_effective = iter1_efficiency * 2**16\n",
    "print(\n",
    "    f\"\\nExtrapolated n_effective_i for 2^16 bank: {iter1_extrapolated_effective:.0f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Second Zoom Iteration\n",
    "\n",
    "Fit a new zoomer from iteration 1 results, compare with iteration 1 zoomer, create a small bank, and run a small PE to check convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load samples from first zoom iteration\n",
    "iter2_post_samples = load_intrinsic_samples_from_rundir(iter1_rundir)\n",
    "iter2_sample_weights = iter2_post_samples[\"weights\"].values\n",
    "\n",
    "iter2_weight_threshold = 0.9\n",
    "iter2_sorted_idx = np.argsort(iter2_sample_weights)[::-1]\n",
    "iter2_cumsum_weights = np.cumsum(iter2_sample_weights[iter2_sorted_idx])\n",
    "iter2_n_selected = np.searchsorted(iter2_cumsum_weights, iter2_weight_threshold) + 1\n",
    "iter2_selected_idx = iter2_sorted_idx[:iter2_n_selected]\n",
    "iter2_top_samples = iter2_post_samples.iloc[iter2_selected_idx]\n",
    "iter2_top_weights = iter2_sample_weights[iter2_selected_idx]\n",
    "\n",
    "print(f\"Selected {iter2_n_selected} samples for iteration 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit iteration 2 zoomer\n",
    "iter2_zoomer, iter2_cond_sampler, iter2_bounds = zoom_iteration.fit_zoomer(\n",
    "    iter2_top_samples, iter2_top_weights, iter1_prior_kwargs, seed=496351, n_sig=1\n",
    ")\n",
    "\n",
    "print(f\"\\nZoomer 1 mean: {iter1_zoomer.mean}\")\n",
    "print(f\"Zoomer 2 mean: {iter2_zoomer.mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two Gaussians using Hellinger distance\n",
    "iter1_zoomer_samples, _ = iter1_zoomer.sample(1000, bounds=iter1_bounds)\n",
    "iter2_zoomer_samples, _ = iter2_zoomer.sample(1000, bounds=iter2_bounds)\n",
    "\n",
    "iter12_hellinger_distance = zoom_iteration.hellinger_distance(\n",
    "    iter1_zoomer_samples,\n",
    "    iter2_zoomer_samples,\n",
    "    iter1_zoomer.distribution.logpdf,\n",
    "    iter2_zoomer.distribution.logpdf,\n",
    ")\n",
    "\n",
    "print(f\"Hellinger distance between zoomer 1 and 2: {iter12_hellinger_distance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom iteration 2 (small bank) setup\n",
    "iter2_zoom_dir = NOTEBOOK_OUTPUT_ROOT / \"zoom\" / \"iter2\"\n",
    "iter2_zoom_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "iter2_small_bank_dir = iter2_zoom_dir / \"bank\"\n",
    "iter2_small_bank_dir.mkdir(parents=True, exist_ok=True)\n",
    "iter2_small_bank_samples_path = iter2_small_bank_dir / \"intrinsic_sample_bank.feather\"\n",
    "iter2_small_bank_config_path = iter2_small_bank_dir / \"bank_config.json\"\n",
    "iter2_small_waveform_dir = iter2_small_bank_dir / \"waveforms\"\n",
    "iter2_small_waveform_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "iter2_small_bank_size = 4096\n",
    "iter2_small_bank_seed = 404\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small bank (4096 samples) from zoomer-conditional-sampling\n",
    "print(\"Creating small bank (4096 samples) from zoomer...\")\n",
    "iter2_small_bank = zoom_iteration.draw_from_zoomer(\n",
    "    iter2_zoomer,\n",
    "    iter2_cond_sampler,\n",
    "    iter2_bounds,\n",
    "    n_samples=iter2_small_bank_size,\n",
    "    seed=iter2_small_bank_seed,\n",
    ")\n",
    "\n",
    "iter2_small_bank.to_feather(iter2_small_bank_samples_path)\n",
    "\n",
    "iter2_small_bank_config = {\n",
    "    \"bank_size\": len(iter2_small_bank),\n",
    "    \"mchirp_min\": initial_mchirp_min,\n",
    "    \"mchirp_max\": initial_mchirp_max,\n",
    "    \"q_min\": initial_q_min,\n",
    "    \"f_ref\": initial_f_ref,\n",
    "    \"fbin\": initial_fbin.tolist(),\n",
    "    \"approximant\": initial_approximant,\n",
    "    \"seed\": iter2_small_bank_seed,\n",
    "}\n",
    "with open(iter2_small_bank_config_path, \"w\") as fp:\n",
    "    json.dump(obj=iter2_small_bank_config, fp=fp, indent=4)\n",
    "print(\n",
    "    f\"Created bank at {iter2_small_bank_samples_path} ({len(iter2_small_bank)} samples)\"\n",
    ")\n",
    "\n",
    "print(\"Generating waveforms...\")\n",
    "waveform_banks.create_waveform_bank_from_samples(\n",
    "    samples_path=iter2_small_bank_samples_path,\n",
    "    bank_config_path=iter2_small_bank_config_path,\n",
    "    waveform_dir=iter2_small_waveform_dir,\n",
    "    n_pool=4,\n",
    "    blocksize=4096,\n",
    "    approximant=initial_approximant,\n",
    ")\n",
    "print(\"Waveforms generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom iteration 2 small inference setup\n",
    "iter2_small_extrinsic_samples_path = Path(iter1_rundir) / \"extrinsic_samples.feather\"\n",
    "\n",
    "iter2_small_inference_kwargs = dict(\n",
    "    event_dir=str(iter2_zoom_dir),\n",
    "    event=initial_inference_event_data,\n",
    "    bank_folder=str(iter2_small_bank_dir),\n",
    "    n_int=len(iter2_small_bank),\n",
    "    n_ext=2048,\n",
    "    n_phi=100,\n",
    "    n_t=128,\n",
    "    blocksize=1024,\n",
    "    single_detector_blocksize=1024,\n",
    "    seed=2001,\n",
    "    size_limit=10**6,\n",
    "    draw_subset=True,\n",
    "    n_draws=1,\n",
    "    extrinsic_samples=str(iter2_small_extrinsic_samples_path),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run zoom iteration 2 small PE\n",
    "iter2_small_rundir = inference.run(**iter2_small_inference_kwargs)\n",
    "print(f\"Zoom iteration 2 small PE complete. Results in {iter2_small_rundir}\")\n",
    "\n",
    "iter2_small_summary_path = Path(iter2_small_rundir) / \"summary_results.json\"\n",
    "iter2_small_run_summary = utils.read_json(iter2_small_summary_path)\n",
    "print(\"\\nZoom iteration 2 small PE results:\")\n",
    "print(f\"  n_effective: {iter2_small_run_summary['n_effective']:.2f}\")\n",
    "print(f\"  n_effective_i: {iter2_small_run_summary['n_effective_i']:.2f}\")\n",
    "print(f\"  n_effective_e: {iter2_small_run_summary['n_effective_e']:.2f}\")\n",
    "print(f\"  Hellinger distance: {iter12_hellinger_distance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom iteration 2 (large bank) setup\n",
    "iter2_large_zoom_dir = NOTEBOOK_OUTPUT_ROOT / \"zoom\" / \"iter2_large\"\n",
    "iter2_large_zoom_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "iter2_large_bank_dir = iter2_large_zoom_dir / \"bank\"\n",
    "iter2_large_bank_dir.mkdir(parents=True, exist_ok=True)\n",
    "iter2_large_bank_samples_path = (\n",
    "    iter2_large_bank_dir / \"intrinsic_sample_bank.feather\"\n",
    ")\n",
    "iter2_large_bank_config_path = iter2_large_bank_dir / \"bank_config.json\"\n",
    "iter2_large_waveform_dir = iter2_large_bank_dir / \"waveforms\"\n",
    "iter2_large_waveform_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "iter2_large_bank_size = 2**16\n",
    "iter2_large_bank_seed = 1984\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Large Bank and Run Full PE\n",
    "\n",
    "Create a large bank (2^16 samples) from iteration 2 zoomer and run full parameter estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large bank (2^16 samples) from zoomer-conditional-sampling\n",
    "print(\"Creating large bank (2^16 samples) from zoomer...\")\n",
    "iter2_large_bank = zoom_iteration.draw_from_zoomer(\n",
    "    iter2_zoomer,\n",
    "    iter2_cond_sampler,\n",
    "    iter2_bounds,\n",
    "    n_samples=iter2_large_bank_size,\n",
    "    seed=iter2_large_bank_seed,\n",
    ")\n",
    "\n",
    "iter2_large_bank.to_feather(iter2_large_bank_samples_path)\n",
    "\n",
    "iter2_large_bank_config = {\n",
    "    \"bank_size\": len(iter2_large_bank),\n",
    "    \"mchirp_min\": initial_mchirp_min,\n",
    "    \"mchirp_max\": initial_mchirp_max,\n",
    "    \"q_min\": initial_q_min,\n",
    "    \"f_ref\": initial_f_ref,\n",
    "    \"fbin\": initial_fbin.tolist(),\n",
    "    \"approximant\": initial_approximant,\n",
    "    \"seed\": iter2_large_bank_seed,\n",
    "}\n",
    "with open(iter2_large_bank_config_path, \"w\") as fp:\n",
    "    json.dump(obj=iter2_large_bank_config, fp=fp, indent=4)\n",
    "\n",
    "print(\n",
    "    f\"Created bank at {iter2_large_bank_samples_path} ({len(iter2_large_bank)} samples)\"\n",
    ")\n",
    "\n",
    "print(\"Generating waveforms...\")\n",
    "waveform_banks.create_waveform_bank_from_samples(\n",
    "    samples_path=iter2_large_bank_samples_path,\n",
    "    bank_config_path=iter2_large_bank_config_path,\n",
    "    waveform_dir=iter2_large_waveform_dir,\n",
    "    n_pool=4,\n",
    "    blocksize=4096,\n",
    "    approximant=initial_approximant,\n",
    ")\n",
    "print(\"Waveforms generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom iteration 2 large inference setup\n",
    "iter2_large_extrinsic_samples_path = Path(iter2_small_rundir) / \"extrinsic_samples.feather\"\n",
    "\n",
    "iter2_large_inference_kwargs = dict(\n",
    "    event_dir=str(iter2_large_zoom_dir),\n",
    "    event=initial_inference_event_data,\n",
    "    bank_folder=str(iter2_large_bank_dir),\n",
    "    n_int=len(iter2_large_bank),\n",
    "    n_ext=2048,\n",
    "    n_phi=100,\n",
    "    n_t=128,\n",
    "    blocksize=1024,\n",
    "    single_detector_blocksize=1024,\n",
    "    seed=2049,\n",
    "    size_limit=10**6,\n",
    "    draw_subset=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run zoom iteration 2 large PE\n",
    "iter2_large_rundir = inference.run(**iter2_large_inference_kwargs)\n",
    "print(f\"Zoom iteration 2 large PE complete. Results in {iter2_large_rundir}\")\n",
    "\n",
    "iter2_large_summary_path = Path(iter2_large_rundir) / \"summary_results.json\"\n",
    "iter2_large_run_summary = utils.read_json(iter2_large_summary_path)\n",
    "print(\"\\nZoom iteration 2 large PE results:\")\n",
    "print(f\"  n_effective: {iter2_large_run_summary['n_effective']:.2f}\")\n",
    "print(f\"  n_effective_i: {iter2_large_run_summary['n_effective_i']:.2f}\")\n",
    "print(f\"  n_effective_e: {iter2_large_run_summary['n_effective_e']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Results\n",
    "\n",
    "Compare intrinsic samples from the initial PE run with the final zoom iteration PE run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare banks\n",
    "\n",
    "initial_bank_dataframe = pd.read_feather(initial_bank_samples_path)\n",
    "iter2_large_bank_dataframe = pd.read_feather(iter2_large_bank_samples_path)\n",
    "\n",
    "for bank_dataframe in [initial_bank_dataframe, iter2_large_bank_dataframe]:\n",
    "    bank_dataframe[\"mchirp\"] = gw_utils.m1m2_to_mchirp(\n",
    "        *bank_dataframe[[\"m1\", \"m2\"]].values.T\n",
    "    )\n",
    "    bank_dataframe[\"lnq\"] = np.log(\n",
    "        np.divide(*bank_dataframe[[\"m1\", \"m2\"]].values.T)\n",
    "    )\n",
    "    bank_dataframe[\"chieff\"] = gw_utils.chieff(\n",
    "        *bank_dataframe[[\"m1\", \"m2\", \"s1z\", \"s2z\"]].values.T\n",
    "    )\n",
    "    bank_dataframe[\"weights\"] = np.exp(bank_dataframe[\"log_prior_weights\"])\n",
    "\n",
    "gw_plotting.MultiCornerPlot(\n",
    "    [initial_bank_dataframe, iter2_large_bank_dataframe],\n",
    "    params=[\"mchirp\", \"lnq\", \"chieff\"],\n",
    "    weights_col=\"None\",\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intrinsic samples from both PE runs\n",
    "initial_run_samples = load_intrinsic_samples_from_rundir(initial_rundir)\n",
    "iter2_large_run_samples = load_intrinsic_samples_from_rundir(iter2_large_rundir)\n",
    "\n",
    "initial_effective_sample_count = initial_run_summary[\"n_effective_i\"]\n",
    "iter2_large_effective_sample_count = iter2_large_run_summary[\"n_effective_i\"]\n",
    "\n",
    "comparison_corner_plot = gw_plotting.MultiCornerPlot(\n",
    "    [initial_run_samples, iter2_large_run_samples],\n",
    "    params=[\"mchirp\", \"lnq\", \"chieff\"],\n",
    "    labels=[\n",
    "        f\"First PE (ESS={initial_effective_sample_count:.2f})\",\n",
    "        f\"After Zoom (ESS={iter2_large_effective_sample_count:.2f})\",\n",
    "    ],\n",
    ")\n",
    "comparison_corner_plot.plot(\n",
    "    legend_title=f\"Full mchirp range {initial_mchirp_min}-{initial_mchirp_max}\"\n",
    ")\n",
    "\n",
    "comparison_param_names = [\"mchirp\", \"lnq\", \"chieff\"]\n",
    "for axis_index, _ in enumerate(comparison_param_names):\n",
    "    for corner_plot in comparison_corner_plot.corner_plots:\n",
    "        ax = corner_plot.axes[axis_index, axis_index]\n",
    "        mean_1d = iter2_zoomer.mean[axis_index]\n",
    "        std_1d = np.sqrt(iter2_zoomer.cov[axis_index, axis_index])\n",
    "\n",
    "        xlim = ax.get_xlim()\n",
    "        x = np.linspace(xlim[0], xlim[1], 200)\n",
    "        rv_1d = norm(mean_1d, std_1d)\n",
    "        pdf_1d = rv_1d.pdf(x)\n",
    "\n",
    "        ylim = ax.get_ylim()\n",
    "        pdf_1d_scaled = pdf_1d * (ylim[1] - ylim[0]) / pdf_1d.max() * 0.8\n",
    "\n",
    "        ax.plot(x, pdf_1d_scaled + ylim[0], \"k-\", linewidth=2, alpha=0.7)\n",
    "\n",
    "comparison_param_pairs = [\n",
    "    ((\"mchirp\", \"lnq\"), (0, 1)),\n",
    "    ((\"mchirp\", \"chieff\"), (0, 2)),\n",
    "    ((\"lnq\", \"chieff\"), (1, 2)),\n",
    "]\n",
    "\n",
    "for (_, _), (i1, i2) in comparison_param_pairs:\n",
    "    for corner_plot in comparison_corner_plot.corner_plots:\n",
    "        ax = corner_plot.axes[i2, i1]\n",
    "        mean_2d = iter2_zoomer.mean[[i1, i2]]\n",
    "        cov_2d = iter2_zoomer.cov[[i1, i2], :][:, [i1, i2]]\n",
    "\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "\n",
    "        x = np.linspace(xlim[0], xlim[1], 200)\n",
    "        y = np.linspace(ylim[0], ylim[1], 200)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        pos = np.dstack((X, Y))\n",
    "\n",
    "        rv = multivariate_normal(mean_2d, cov_2d)\n",
    "        Z = rv.pdf(pos)\n",
    "\n",
    "        levels = [rv.pdf(mean_2d) * np.exp(-0.5 * v**2) for v in [2, 1]]\n",
    "\n",
    "        ax.contour(\n",
    "            X,\n",
    "            Y,\n",
    "            Z,\n",
    "            levels=[levels[0]],\n",
    "            colors=[\"gray\"],\n",
    "            linestyles=[\"--\"],\n",
    "            linewidths=1.5,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax.contour(\n",
    "            X,\n",
    "            Y,\n",
    "            Z,\n",
    "            levels=[levels[1]],\n",
    "            colors=[\"black\"],\n",
    "            linestyles=[\"-\"],\n",
    "            linewidths=1.5,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "handles, labels = (\n",
    "    comparison_corner_plot.corner_plots[0].axes[0][0].get_legend_handles_labels()\n",
    ")\n",
    "gaussian_line = Line2D(\n",
    "    [0],\n",
    "    [0],\n",
    "    color=\"black\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"-\",\n",
    "    label=\"Gaussian fit to First PE\",\n",
    ")\n",
    "handles.insert(1, gaussian_line)\n",
    "labels.insert(1, \"Gaussian fit to First PE\")\n",
    "comparison_corner_plot.corner_plots[0].axes[0][-1].legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    bbox_to_anchor=(1, 1),\n",
    "    frameon=False,\n",
    "    loc=\"upper right\",\n",
    "    borderaxespad=0,\n",
    "    borderpad=0,\n",
    ")\n",
    "\n",
    "injection_reference_point = {\n",
    "    \"mchirp\": injection_mchirp,\n",
    "    \"lnq\": np.log(injection_mass_ratio),\n",
    "    \"chieff\": gw_utils.chieff(\n",
    "        injection_parameters[\"m1\"],\n",
    "        injection_parameters[\"m2\"],\n",
    "        injection_parameters[\"s1z\"],\n",
    "        injection_parameters[\"s2z\"],\n",
    "    ),\n",
    "}\n",
    "comparison_corner_plot.scatter_points(injection_reference_point, colors=\"r\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dot-pe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
