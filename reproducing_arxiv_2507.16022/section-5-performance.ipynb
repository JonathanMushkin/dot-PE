{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 5: Performance Analysis - arXiv:2507.16022\n",
        "\n",
        "This notebook reproduces the performance analysis from Section 5 of **\"Sampler-free gravitational wave inference using matrix multiplication\"**.\n",
        "\n",
        "The analysis covers:\n",
        "1. **Integration Performance**: Speed and efficiency of the dot-PE method vs traditional samplers\n",
        "2. **Parameter Estimation Performance**: Accuracy and convergence behavior across different bank densities and parameter ranges\n",
        "\n",
        "This notebook processes results from:\n",
        "- Reference runs (dense banks) - establishing ground truth\n",
        "- Convergence runs (regular banks) - testing parameter variations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Scientific computing\n",
        "import scipy.stats\n",
        "import scipy.ndimage\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Import packages\n",
        "from cogwheel import data, utils, gw_plotting, gw_utils\n",
        "from dot_pe import inference\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Plotting style\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bank_names = [f\"bank_mchirp_{x}\" for x in [3, 5, 10, 20, 50, 100]]\n",
        "dense_bank_names = [f\"bank_mchirp_dense_{x}\" for x in [3, 5, 10, 20, 50, 100]]\n",
        "banks_homedir = Path(\n",
        "    \"/home/projects/barakz/Collaboration-gw/mushkin//magic_integral/banks/\"\n",
        ")\n",
        "\n",
        "bank_folders = [banks_homedir / bank_name for bank_name in bank_names]\n",
        "dense_bank_folders = [\n",
        "    banks_homedir / bank_name for bank_name in dense_bank_names\n",
        "]\n",
        "\n",
        "bank_configs = [\n",
        "    utils.read_json(bank_folder / \"bank_config.json\")\n",
        "    for bank_folder in bank_folders\n",
        "]\n",
        "\n",
        "mass_ranges = {\n",
        "    bank_name: (bank_config[\"min_mchirp\"], bank_config[\"max_mchirp\"])\n",
        "    for bank_name, bank_config in zip(bank_names, bank_configs)\n",
        "}\n",
        "\n",
        "bank_labels = {\n",
        "    bank_name: r\"$\\mathcal{M} \\in (\"\n",
        "    + f\"{mass_ranges[bank_name][0]:.3g},{mass_ranges[bank_name][1]:.3g}\"\n",
        "    + r\"){\\rm M}_{\\odot}$\"\n",
        "    for bank_name in bank_names\n",
        "}\n",
        "markers = [\"o\", \"s\", \"D\", \"^\", \"v\", \"P\"]  # Define 6 different markers\n",
        "bank_markers = {\n",
        "    bank_name: marker for bank_name, marker in zip(bank_names, markers)\n",
        "}\n",
        "color_map = plt.colormaps[\"tab10\"]\n",
        "bank_colors = {bank: color_map(i) for i, bank in enumerate(bank_names)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pickle(path):\n",
        "    try:\n",
        "        if Path(path).exists():\n",
        "            with open(path, \"rb\") as fp:\n",
        "                return pickle.load(fp)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    except (pickle.UnpicklingError, FileNotFoundError, IOError) as e:\n",
        "        print(f\"Error loading pickle file: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_joint_df_of_injections():\n",
        "    dfs = []\n",
        "    for bank_name in bank_names:\n",
        "        df = pd.read_feather(f\"../{bank_name}/injections.feather\")\n",
        "        df[\"eventname\"] = [f\"injection_{bank_name}_{i:04}\" for i in range(len(df))]\n",
        "        df[\"bank_name\"] = bank_name\n",
        "        dfs.append(df)\n",
        "    return pd.concat(dfs).reset_index(inplace=False, drop=True)\n",
        "\n",
        "\n",
        "def update_on_eventname(df1, df2):\n",
        "    common = list(set(df1[\"eventname\"]) & set(df2[\"eventname\"]))\n",
        "    df1 = df1[df1[\"eventname\"].isin(common)].set_index(\"eventname\")\n",
        "    df2 = df2[df2[\"eventname\"].isin(common)].set_index(\"eventname\")\n",
        "\n",
        "    updated = df1.combine_first(df2)\n",
        "    updated = updated.loc[common]  # ensure output is limited to common eventnames\n",
        "    updated.reset_index(inplace=True)\n",
        "    return updated\n",
        "\n",
        "\n",
        "def fill_df_with_params(df, inplace=True):\n",
        "    if not inplace:\n",
        "        df = df.copy()\n",
        "    df[\"mchirp\"] = gw_utils.m1m2_to_mchirp(df[\"m1\"], df[\"m2\"])\n",
        "    df[\"chieff\"] = gw_utils.chieff(df[\"m1\"], df[\"m2\"], df[\"s1z\"], df[\"s2z\"])\n",
        "    df[\"eta\"] = gw_utils.q_to_eta(df[\"m2\"] / df[\"m1\"])\n",
        "    df[\"lnq\"] = np.log(df[\"m2\"] / df[\"m1\"])\n",
        "    if not inplace:\n",
        "        return df\n",
        "\n",
        "\n",
        "def sort_path_list(l):\n",
        "    return sorted(l, key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
        "\n",
        "\n",
        "def load_condition(rundir):\n",
        "    \"\"\"Return True/False if to load/not load results from a rundir\"\"\"\n",
        "    rundir = Path(rundir)\n",
        "    if not (rundir / \"run_kwargs.json\").exists():\n",
        "        return False\n",
        "    if not (rundir / \"summary_results.json\").exists():\n",
        "        return False\n",
        "    if not (rundir / \"samples.feather\").exists():\n",
        "        return False\n",
        "    # summary_results =  utils.read_json(rundir / \"summary_results.json\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_mchirp_quantiles(\n",
        "    rundir: Path, quantiles: list[float] = [0.05, 0.95], n_min: int = 20\n",
        ") -> np.ndarray:\n",
        "    rundir = Path(rundir)\n",
        "    default_output = np.array([np.nan, np.nan], dtype=float)\n",
        "    samples_path = rundir / \"samples.feather\"\n",
        "    if not samples_path.exists():\n",
        "        return default_output\n",
        "    samples = pd.read_feather(samples_path)\n",
        "    if len(samples) < n_min:\n",
        "        return default_output\n",
        "    qunatiles = samples[\"mchirp\"].quantile(quantiles).to_numpy(dtype=float)\n",
        "    if np.diff(quantiles) == 0:\n",
        "        return default_output\n",
        "    return qunatiles\n",
        "\n",
        "\n",
        "def load_results(bank_names, inj_dfs=None):\n",
        "    rows = []\n",
        "    homedir = Path(\".\")  # Current directory contains the results\n",
        "    for b, bank_name in enumerate(bank_names):\n",
        "        if inj_dfs is None:\n",
        "            inj_df = pd.read_feather(homedir / bank_name / \"injections.feather\")\n",
        "        else:\n",
        "            inj_df = inj_dfs[b]\n",
        "        inj_df[\"eventname\"] = [\n",
        "            f\"injection_{bank_name.replace('_dense_', '_')}_{i:04}\"\n",
        "            for i in range(len(inj_df))\n",
        "        ]\n",
        "        inj_df[\"bank_name\"] = bank_name\n",
        "\n",
        "        event_dirs = sort_path_list((homedir / f\"{bank_name}_events\").glob(\"injection_*\"))\n",
        "        for event_dir in tqdm(event_dirs, desc=bank_name):\n",
        "            eventname = event_dir.name\n",
        "            pickle_path = event_dir / \"fisher_analysis\" / \"results.pkl\"\n",
        "            if (pickle_path).exists():\n",
        "                fisher_results = load_pickle(pickle_path)\n",
        "                fisher_results = {\n",
        "                    k: fisher_results[k]\n",
        "                    for k in [\n",
        "                        \"sigma_Mc_from_fisher\",\n",
        "                        \"FIM_inversion_error\",\n",
        "                    ]\n",
        "                }\n",
        "            else:\n",
        "                fisher_results = {}\n",
        "\n",
        "            rundirs = sort_path_list(event_dir.glob(\"run_*\"))\n",
        "            for rundir in rundirs:\n",
        "                if load_condition(rundir):\n",
        "                    summary_results = utils.read_json(rundir / \"summary_results.json\")\n",
        "\n",
        "                    run_kwargs = utils.read_json(rundir / \"run_kwargs.json\")\n",
        "                    run_kwargs.pop(\"seed\", None)  # Remove seed if present\n",
        "\n",
        "                    par_dic = (\n",
        "                        inj_df.loc[inj_df.eventname == eventname].iloc[0].to_dict()\n",
        "                    )\n",
        "                    row = (\n",
        "                        run_kwargs\n",
        "                        | summary_results\n",
        "                        | par_dic\n",
        "                        | dict(\n",
        "                            rundir=str(rundir),\n",
        "                            eventname=event_dir.name,\n",
        "                            dense_bank_name=bank_name,\n",
        "                        )\n",
        "                        | fisher_results\n",
        "                    )\n",
        "                    rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "\n",
        "def plot_2d_hist(\n",
        "    ax, dfs, params, colors=None, smooth=1.0, labels=None, weights_col=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a 2D histogram plot similar to MultiCornerPlot but for a single panel.\n",
        "\n",
        "    Parameters\n",
        "\n",
        "    ----------\n",
        "    ax : matplotlib.axes.Axes\n",
        "        The axis to plot on\n",
        "    dfs : list of pandas.DataFrame\n",
        "        List of dataframes containing the data for each overlay\n",
        "    params : tuple of str\n",
        "        (x_param, y_param) names of columns to plot\n",
        "    colors : list of str, optional\n",
        "        Colors for each dataset. If None, will use default color cycle\n",
        "    smooth : float, default=1.0\n",
        "        Smoothing factor for the gaussian filter\n",
        "    labels : list of str, optional\n",
        "        Labels for each dataset for the legend\n",
        "    weights_col : str, optional\n",
        "        Name of the column containing weights. If None, no weights are used.\n",
        "    \"\"\"\n",
        "\n",
        "    if colors is None:\n",
        "        # Generate colors exactly as in the original implementation\n",
        "        tab20_colors = plt.cm.tab20.colors\n",
        "        tab20b_colors = plt.cm.tab20b.colors\n",
        "        all_colors = (\n",
        "            tab20_colors[::2]\n",
        "            + tab20_colors[1::2]\n",
        "            + tab20b_colors[::2]\n",
        "            + tab20b_colors[1::2]\n",
        "        )\n",
        "        colors = [all_colors[i % len(all_colors)] for i in range(len(dfs))]\n",
        "\n",
        "    # Contour levels for 90% and 50% containment (in decreasing order as in original)\n",
        "    contour_fractions = [0.9, 0.5]\n",
        "\n",
        "    for i, df in enumerate(dfs):\n",
        "        xpar, ypar = params\n",
        "\n",
        "        # Get weights if specified\n",
        "        weights = df.get(weights_col) if weights_col else None\n",
        "\n",
        "        # Calculate number of bins using Rice rule\n",
        "        if weights is None:\n",
        "            n_effective = len(df)\n",
        "        else:\n",
        "            # Calculate effective sample size for weighted data\n",
        "            n_effective = np.square(np.sum(weights)) / np.sum(np.square(weights))\n",
        "        n_bins = int(np.ceil(2 * np.cbrt(n_effective)))\n",
        "        valid_mask = df[[xpar, ypar]].notna().all(axis=1)\n",
        "        df = df[valid_mask]\n",
        "        if len(df) == 0:\n",
        "            continue\n",
        "        # Calculate 2D histogram\n",
        "        hist2d, xedges, yedges = np.histogram2d(\n",
        "            df[xpar], df[ypar], bins=n_bins, weights=weights\n",
        "        )\n",
        "\n",
        "        # Apply gaussian smoothing\n",
        "        hist2d = scipy.ndimage.gaussian_filter(hist2d, smooth)\n",
        "\n",
        "        # Get midpoints for interpolation\n",
        "        x_mid = (xedges[1:] + xedges[:-1]) / 2\n",
        "        y_mid = (yedges[1:] + yedges[:-1]) / 2\n",
        "\n",
        "        # Interpolate to get PDF at edges\n",
        "        pdf = scipy.interpolate.RectBivariateSpline(x_mid, y_mid, hist2d)(\n",
        "            xedges, yedges\n",
        "        ).T\n",
        "\n",
        "        # Calculate levels for contours\n",
        "        sorted_pdf = [0.0] + sorted(pdf.ravel())\n",
        "        cdf = np.cumsum(sorted_pdf)\n",
        "        cdf /= cdf[-1]\n",
        "        ccdf = 1 - cdf\n",
        "        levels = np.interp(contour_fractions, ccdf[::-1], sorted_pdf[::-1])\n",
        "        print(f\"  PDF max: {pdf.max()}, levels: {levels}\")\n",
        "\n",
        "        # Plot contours\n",
        "        extent = (\n",
        "            df[xpar].min(),\n",
        "            df[xpar].max(),\n",
        "            df[ypar].min(),\n",
        "            df[ypar].max(),\n",
        "        )\n",
        "\n",
        "        # Draw contours\n",
        "        contour = ax.contour(\n",
        "            pdf,\n",
        "            extent=extent,\n",
        "            levels=levels,\n",
        "            colors=[colors[i]],\n",
        "            linestyles=[\"-\"],\n",
        "        )\n",
        "\n",
        "        # Fill contours with transparency\n",
        "        alphas = 1 - np.array(\n",
        "            contour_fractions\n",
        "        )  # Transparency increases with confidence\n",
        "        next_levels = [*levels[1:], np.inf]\n",
        "        for level_edges, alpha in zip(zip(levels, next_levels), alphas):\n",
        "            ax.contourf(\n",
        "                pdf,\n",
        "                extent=extent,\n",
        "                levels=level_edges,\n",
        "                colors=[colors[i]],\n",
        "                alpha=alpha,\n",
        "            )\n",
        "\n",
        "    if labels is not None:\n",
        "        ax.legend(labels, frameon=False, loc=\"upper right\")\n",
        "\n",
        "    # Set axis labels\n",
        "    ax.set_xlabel(params[0])\n",
        "    ax.set_ylabel(params[1])\n",
        "\n",
        "    # Add ticks on all sides and rotate them\n",
        "    ax.tick_params(which=\"both\", direction=\"in\", right=True, top=True, rotation=45)\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def get_rms(x):\n",
        "    return np.sqrt(np.mean(x**2))\n",
        "\n",
        "\n",
        "def get_abs(x):\n",
        "    return np.abs(x).item()\n",
        "\n",
        "\n",
        "def return_self(x):\n",
        "    return x\n",
        "\n",
        "def process_subplot(\n",
        "    ax,\n",
        "    df,\n",
        "    x_col,\n",
        "    y_col,\n",
        "    bank_names,\n",
        "    labels,\n",
        "    markers,\n",
        "    func=lambda x: x,\n",
        "    min_counts_for_display=10,\n",
        "    xscale=\"log\",\n",
        "    yscale=\"log\",\n",
        "):\n",
        "    for b, bank_name in enumerate(bank_names):\n",
        "        color = bank_colors[bank_name]\n",
        "        all_lines = []\n",
        "        filtered = df[df[\"eventname\"].str.contains(f\"{bank_name}_\", na=False)]\n",
        "\n",
        "        for eventname in filtered[\"eventname\"].unique():\n",
        "            cut = filtered[filtered[\"eventname\"] == eventname]\n",
        "\n",
        "            if cut.empty:\n",
        "                continue\n",
        "\n",
        "            # Apply func row-wise to y_col\n",
        "            x_vals = cut[x_col].to_numpy()\n",
        "            y_vals = cut[y_col].apply(func).to_numpy()\n",
        "\n",
        "            if len(x_vals) == 0 or len(y_vals) == 0:\n",
        "                continue\n",
        "\n",
        "            all_lines.append((x_vals, y_vals))\n",
        "\n",
        "        if not all_lines:\n",
        "            continue\n",
        "\n",
        "        x_common = np.geomspace(\n",
        "            min(x.min() for x, _ in all_lines),\n",
        "            max(x.max() for x, _ in all_lines),\n",
        "            10,\n",
        "        )\n",
        "        y_interp = []\n",
        "        for x, y in all_lines:\n",
        "            f = interp1d(x, y, bounds_error=False, fill_value=np.nan)\n",
        "            y_interp.append(f(x_common))\n",
        "        y_interp = np.array(y_interp)\n",
        "\n",
        "        y_median = np.nanmedian(y_interp, axis=0)\n",
        "        y_q25 = np.nanpercentile(y_interp, 25, axis=0)\n",
        "        y_q75 = np.nanpercentile(y_interp, 75, axis=0)\n",
        "\n",
        "        valid_counts = np.sum(~np.isnan(y_interp), axis=0)\n",
        "        if min_counts_for_display:\n",
        "            cond = valid_counts >= min_counts_for_display\n",
        "            y_median[~cond] = np.nan\n",
        "            y_q25[~cond] = np.nan\n",
        "            y_q75[~cond] = np.nan\n",
        "\n",
        "        ax.plot(\n",
        "            x_common,\n",
        "            y_median,\n",
        "            color=color,\n",
        "            lw=2,\n",
        "            label=labels[bank_name],\n",
        "            marker=markers[b],\n",
        "        )\n",
        "        ax.fill_between(x_common, y_q25, y_q75, color=color, alpha=0.1)\n",
        "\n",
        "    ax.set_xscale(xscale)\n",
        "    ax.set_yscale(yscale)\n",
        "    ax.grid()\n",
        "\n",
        "\n",
        "def get_crossing_points_from_ax(ax, y_threshold):\n",
        "    results = []\n",
        "\n",
        "    for coll in ax.collections:\n",
        "        if not hasattr(coll, \"get_paths\"):\n",
        "            continue\n",
        "\n",
        "        paths = coll.get_paths()\n",
        "        if not paths:\n",
        "            continue\n",
        "\n",
        "        verts = paths[0].vertices\n",
        "        n = len(verts) // 2\n",
        "\n",
        "        x = verts[:n, 0]\n",
        "        y_lower = verts[:n, 1]\n",
        "        y_upper = verts[n:, 1][\n",
        "            ::-1\n",
        "        ]  # reversed: upper bound goes right-to-left\n",
        "\n",
        "        # Find first x where upper bound drops below threshold\n",
        "        below = y_upper < y_threshold\n",
        "        if np.any(below):\n",
        "            idx = np.argmax(below)\n",
        "        else:\n",
        "            idx = len(x) - 1\n",
        "\n",
        "        results.append(x[idx])\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create key dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "injections = [\n",
        "    pd.read_feather(f\"{bank_name}_events/injections.feather\")\n",
        "    for bank_name in bank_names\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dense_bank_results = load_results(dense_bank_names, injections)\n",
        "dense_bank_results[[\"mchirp_005\", \"mchirp_095\"]] = pd.DataFrame(\n",
        "    dense_bank_results[\"rundir\"].apply(get_mchirp_quantiles).tolist(),\n",
        "    columns=[\"mchirp_005\", \"mchirp_095\"],\n",
        ")\n",
        "dense_bank_results.to_feather(\"dense_bank_results.feather\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print the number of events successfuly done in each bank\n",
        "for bank_name in dense_bank_results.bank_name.unique():\n",
        "    cond = dense_bank_results.bank_name == bank_name\n",
        "    print(bank_name, len(dense_bank_results.loc[cond].eventname.unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## reference dataframe\n",
        "# take > 10 mintues to run, prefer to load it\n",
        "_t = time.time()\n",
        "bank_results = load_results(bank_names, injections)\n",
        "\n",
        "_t = time.time() - _t\n",
        "print(f\"passed {_t / 60:.3g} minutes\")\n",
        "\n",
        "bank_results.to_feather(\"standard_bank_results.feather\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## load key dataframes from files\n",
        "\n",
        "injections = [\n",
        "    pd.read_feather(f\"{bank_name}_events/injections.feather\")\n",
        "    for bank_name in bank_names\n",
        "]\n",
        "\n",
        "bank_results = pd.read_feather(\"standard_bank_results.feather\")\n",
        "dense_bank_results = pd.read_feather(\"dense_bank_results.feather\")\n",
        "\n",
        "bank_results.drop([\"n_draws\", \"inds_path\", \"delete_blocks\"], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "dense_bank_results.drop([\"n_draws\", \"inds_path\"], axis=1, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Processing\n",
        "\n",
        "This section loads results from both reference runs (dense banks) and convergence runs (regular banks), then processes them for performance analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the merged dataframe with reference results (ground truth)\n",
        "# This combines convergence runs with dense bank reference runs\n",
        "\n",
        "# Setup for dropping duplicates - define which columns identify unique runs\n",
        "kwargs_setup_dense = [\"bank_name\", \"eventname\", \"n_int\", \"n_ext\"]\n",
        "kwargs_setup_regular = [\"bank_name\", \"eventname\", \"n_int\", \"n_ext\"] \n",
        "\n",
        "# Create unique reference results from dense banks (ground truth)\n",
        "unique_reference_results = dense_bank_results.drop_duplicates(\n",
        "    subset=kwargs_setup_dense, keep=\"first\"\n",
        ").copy()\n",
        "\n",
        "unique_reference_results.rename(\n",
        "    columns={\n",
        "        \"rundir\": \"rundir_dense_bank\", \n",
        "        \"ln_evidence\": \"ln_evidence_reference\",\n",
        "        \"n_i_inds_used\": \"n_i_inds_used_dense_bank\",\n",
        "        \"n_effective\": \"n_effective_dense_bank\",\n",
        "        \"n_effective_i\": \"n_effective_i_dense_bank\", \n",
        "        \"n_effective_e\": \"n_effective_e_dense_bank\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "unique_reference_results = unique_reference_results[[\n",
        "    \"rundir_dense_bank\",\n",
        "    \"eventname\", \n",
        "    \"bank_name\",\n",
        "    \"ln_evidence_reference\",\n",
        "    \"n_i_inds_used_dense_bank\",\n",
        "    \"n_effective_dense_bank\",\n",
        "    \"n_effective_i_dense_bank\",\n",
        "    \"n_effective_e_dense_bank\",\n",
        "]].copy()  # drop other columns to avoid confusion\n",
        "\n",
        "# Create unique regular bank results \n",
        "unique_bank_results = bank_results.drop_duplicates(\n",
        "    subset=kwargs_setup_regular, keep=\"first\"  \n",
        ").copy()\n",
        "\n",
        "# Merge convergence results with reference results\n",
        "bank_results_with_reference = update_on_eventname(\n",
        "    df1=unique_bank_results, df2=unique_reference_results\n",
        ")\n",
        "\n",
        "# Calculate derived quantities for analysis\n",
        "bank_results_with_reference[\"n_effective_harmonic\"] = 2 / (\n",
        "    1 / bank_results_with_reference[\"n_effective_i\"] \n",
        "    + 1 / bank_results_with_reference[\"n_effective_e\"]\n",
        ")\n",
        "\n",
        "bank_results_with_reference[\"ln_evidence_error\"] = (\n",
        "    bank_results_with_reference[\"ln_evidence\"] \n",
        "    - bank_results_with_reference[\"ln_evidence_reference\"]\n",
        ")\n",
        "\n",
        "bank_results_with_reference[\"abs_ln_evidence_error\"] = np.abs(\n",
        "    bank_results_with_reference[\"ln_evidence_error\"]\n",
        ")\n",
        "\n",
        "bank_results_with_reference[\"norm_error\"] = (\n",
        "    np.exp(bank_results_with_reference[\"ln_evidence_error\"]) - 1\n",
        ")\n",
        "bank_results_with_reference[\"norm_abs_error\"] = np.abs(\n",
        "    bank_results_with_reference[\"norm_error\"]\n",
        ")\n",
        "\n",
        "print(f\"Created bank_results_with_reference with {len(bank_results_with_reference)} rows\")\n",
        "print(\"Available columns:\", list(bank_results_with_reference.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure 2: Convergence Analysis\n",
        "\n",
        "Now we recreate Figure 2 from the paper, showing convergence behavior of ln Z accuracy as a function of intrinsic samples (N_int), preselected intrinsic samples (N_int'), and extrinsic samples (N_ext).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(12, 4), sharey=True)\n",
        "axs = axs.flatten()\n",
        "min_counts_for_display = 100\n",
        "label_fontsize = 14\n",
        "legend_fontsize = 10\n",
        "legend_title_fontsize = 11\n",
        "\n",
        "for ax in axs:\n",
        "    ax.axhline(\n",
        "        1,\n",
        "        ls=\"--\",\n",
        "        c=\"k\",\n",
        "        label=r\"$\\ln\\mathcal{Z}=\\ln\\hat\\mathcal{Z}\\pm1$\",\n",
        "    )\n",
        "\n",
        "common_y_col = \"abs_ln_evidence_error\"\n",
        "common_y_label = (\n",
        "    r\"$\\left\\vert \\ln\\mathcal{Z} - \\ln\\hat{\\mathcal{Z}} \\right\\vert$\"\n",
        ")\n",
        "\n",
        "# Panel 1: N_int vs ln Z accuracy (N_ext fixed to 1024)\n",
        "ax = axs[0]\n",
        "process_subplot(\n",
        "    ax,\n",
        "    bank_results_with_reference.loc[\n",
        "        bank_results_with_reference[\"n_ext\"] == 1024\n",
        "    ],\n",
        "    x_col=\"n_int\",\n",
        "    y_col=common_y_col,\n",
        "    bank_names=bank_names,\n",
        "    labels=bank_labels,\n",
        "    markers=markers,\n",
        "    func=return_self,\n",
        "    min_counts_for_display=min_counts_for_display,\n",
        ")\n",
        "ax.set_xlabel(r\"$N_{\\rm int.}$\", fontsize=label_fontsize)\n",
        "ax.set_ylabel(common_y_label, fontsize=label_fontsize)\n",
        "\n",
        "# Panel 2: N_int' (preselected) vs ln Z accuracy (N_ext fixed to 1024)\n",
        "ax = axs[1]\n",
        "process_subplot(\n",
        "    ax,\n",
        "    bank_results_with_reference.loc[\n",
        "        bank_results_with_reference[\"n_ext\"] == 1024\n",
        "    ],\n",
        "    x_col=\"n_i_inds_used\",\n",
        "    y_col=common_y_col,\n",
        "    bank_names=bank_names,\n",
        "    labels=bank_labels,\n",
        "    markers=markers,\n",
        "    func=return_self,\n",
        "    min_counts_for_display=min_counts_for_display,\n",
        ")\n",
        "ax.set_xlabel(r\"$N_{\\rm int.}'$\", fontsize=label_fontsize)\n",
        "\n",
        "# Panel 3: N_ext vs ln Z accuracy (N_int fixed to 2^16)\n",
        "ax = axs[2]\n",
        "process_subplot(\n",
        "    ax,\n",
        "    bank_results_with_reference.loc[\n",
        "        (bank_results_with_reference[\"n_int\"] == 2**16)\n",
        "        * (bank_results_with_reference[\"n_ext\"] <= 1024)\n",
        "    ],\n",
        "    x_col=\"n_ext\",\n",
        "    y_col=common_y_col,\n",
        "    bank_names=bank_names,\n",
        "    labels=bank_labels,\n",
        "    markers=markers,\n",
        "    func=return_self,\n",
        "    min_counts_for_display=min_counts_for_display,\n",
        ")\n",
        "ax.set_xlabel(r\"$N_{\\rm ext.}$\", fontsize=label_fontsize)\n",
        "leg = ax.legend(fontsize=legend_fontsize, bbox_to_anchor=(1.05, 0.975))\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"convergence_n_int_n_ext.pdf\", format=\"pdf\")\n",
        "\n",
        "print(\"Figure 2: Convergence analysis complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure 4: Effective Sample Size Analysis\n",
        "\n",
        "Now we recreate Figure 4 from the paper, showing the relationship between effective sample sizes (N_eff_int, N_eff_ext, and their harmonic mean) and inference accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 4: Three-panel n_effective plot - effective samples vs ln Z accuracy \n",
        "fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(12, 4), sharey=True)\n",
        "\n",
        "min_n_eff_i = 32\n",
        "min_n_eff_e = 32\n",
        "min_counts_for_display = 100\n",
        "label_fontsize = 14\n",
        "legend_fontsize = 10\n",
        "legend_title_fontsize = 11\n",
        "y_label = r\"$\\left \\vert \\mathcal{Z}/\\hat\\mathcal{Z}-1\\right\\vert$\"\n",
        "\n",
        "# Panel 1: n_effective_int vs accuracy (n_ext >= min threshold)\n",
        "ax = axs[0]\n",
        "cond = bank_results_with_reference[\"n_ext\"] >= min_n_eff_e\n",
        "common_y_col = \"norm_abs_error\"\n",
        "process_subplot(\n",
        "    ax,\n",
        "    bank_results_with_reference.loc[cond],\n",
        "    x_col=\"n_effective_i\",\n",
        "    y_col=common_y_col,\n",
        "    bank_names=bank_names,\n",
        "    labels=bank_labels,\n",
        "    markers=markers,\n",
        "    func=get_abs,\n",
        "    min_counts_for_display=min_counts_for_display,\n",
        "    xscale=\"log\",\n",
        "    yscale=\"log\",\n",
        ")\n",
        "ax.set_xlabel(r\"$N_{\\rm eff. int.}$\", fontsize=label_fontsize)\n",
        "ax.set_ylabel(y_label, fontsize=label_fontsize)\n",
        "\n",
        "# Panel 2: n_effective_ext vs accuracy (n_int >= min threshold)\n",
        "ax = axs[1]\n",
        "process_subplot(\n",
        "    ax,\n",
        "    bank_results_with_reference.loc[\n",
        "        bank_results_with_reference[\"n_effective_i\"] >= min_n_eff_i\n",
        "    ],\n",
        "    x_col=\"n_effective_e\",\n",
        "    y_col=common_y_col,\n",
        "    bank_names=bank_names,\n",
        "    labels=bank_labels,\n",
        "    markers=markers,\n",
        "    func=get_abs,\n",
        "    min_counts_for_display=min_counts_for_display,\n",
        ")\n",
        "ax.set_xlabel(r\"$N_{\\rm eff. ext.}$\", fontsize=label_fontsize)\n",
        "\n",
        "# Panel 3: harmonic mean of n_effective vs accuracy\n",
        "ax = axs[2]\n",
        "process_subplot(\n",
        "    ax,\n",
        "    bank_results_with_reference,\n",
        "    x_col=\"n_effective_harmonic\",\n",
        "    y_col=common_y_col,\n",
        "    bank_names=bank_names,\n",
        "    labels=bank_labels,\n",
        "    markers=markers,\n",
        "    func=get_abs,\n",
        "    min_counts_for_display=min_counts_for_display,\n",
        ")\n",
        "ax.set_xlabel(\n",
        "    r\"Harmonic Mean $(N_{\\rm eff. int.},N_{\\rm eff. ext.})$\",\n",
        "    fontsize=label_fontsize,\n",
        ")\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_yscale(\"log\")\n",
        "\n",
        "# Add legend to rightmost panel\n",
        "ax = axs[2]\n",
        "leg = ax.legend(fontsize=legend_fontsize, bbox_to_anchor=(1.05, 0.975))\n",
        "frame = leg.get_frame()\n",
        "bbox_dict = {\n",
        "    \"boxstyle\": frame.get_boxstyle().__class__.__name__.lower(),\n",
        "    \"facecolor\": frame.get_facecolor(),\n",
        "    \"edgecolor\": frame.get_edgecolor(),\n",
        "    \"linewidth\": frame.get_linewidth(),\n",
        "}\n",
        "\n",
        "# Add condition text to first two panels\n",
        "ax = axs[0]\n",
        "ax.text(\n",
        "    0.60,\n",
        "    0.95,\n",
        "    r\"$N_{\\rm eff. ext.}\\geq \" + str(min_n_eff_e) + r\"$\",\n",
        "    transform=ax.transAxes,\n",
        "    fontsize=legend_title_fontsize,\n",
        "    verticalalignment=\"top\",\n",
        "    bbox=bbox_dict,\n",
        ")\n",
        "\n",
        "ax = axs[1]\n",
        "ax.text(\n",
        "    0.60,\n",
        "    0.95,\n",
        "    r\"$N_{\\rm eff. int.}\\geq\" + str(min_n_eff_i) + r\"$\",\n",
        "    transform=ax.transAxes,\n",
        "    fontsize=legend_title_fontsize,\n",
        "    verticalalignment=\"top\",\n",
        "    bbox=bbox_dict,\n",
        ")\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"n_effective_delta_ln_Z.pdf\", format=\"pdf\")\n",
        "\n",
        "print(\"Figure 4: N_effective analysis complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure 5: Selection and Effective Sample Size Analysis\n",
        "\n",
        "This section processes dense bank results to analyze the relationship between N'_int (after incoherent likelihood selection), effective intrinsic samples size N_eff_int, and M confidence-interval prior integral.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for processing dense bank results\n",
        "\n",
        "def make_event_dataframe(event_dir: Path):\n",
        "    rows = []\n",
        "    event_dir = Path(event_dir)\n",
        "    rundirs = sorted(\n",
        "        event_dir.glob(\"run_*\"), key=lambda x: int(x.name.split(\"_\")[-1])\n",
        "    )\n",
        "    for rundir in rundirs:\n",
        "        samples_path = rundir / \"samples.feather\"\n",
        "        run_kwargs_path = rundir / \"run_kwargs.json\"\n",
        "        summary_results_path = rundir / \"summary_results.json\"\n",
        "        if (\n",
        "            samples_path.exists()\n",
        "            and run_kwargs_path.exists()\n",
        "            and summary_results_path.exists()\n",
        "        ):\n",
        "            n_samples = len(pd.read_feather(samples_path))\n",
        "            summary_results = utils.read_json(summary_results_path)\n",
        "            row = dict(\n",
        "                n_effective=summary_results[\"n_effective\"],\n",
        "                n_effective_i=summary_results[\"n_effective_i\"],\n",
        "                n_int_prime=summary_results[\"n_i_inds_used\"],\n",
        "                n_int=utils.read_json(run_kwargs_path)[\"n_int\"],\n",
        "                n_samples=n_samples,\n",
        "                rundir=str(rundir),\n",
        "                event_dir=str(event_dir),\n",
        "                ln_evidence=summary_results[\"ln_evidence\"],\n",
        "            )\n",
        "        else:\n",
        "            row = {}\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_bank_results_dense(bank_folder: Path) -> pd.DataFrame:\n",
        "    failed_events = []\n",
        "    event_dirs = sorted(\n",
        "        Path(bank_folder).glob(\"injection_bank_mchirp*\"),\n",
        "        key=lambda x: int(x.name.split(\"_\")[-1]),\n",
        "    )\n",
        "    rows = []\n",
        "    for event_dir in tqdm(event_dirs, total=len(event_dirs)):\n",
        "        df = make_event_dataframe(event_dir)\n",
        "        if len(df) == 0 or df.size == 0:\n",
        "            failed_events.append(event_dir)\n",
        "            continue\n",
        "        df = df.sort_values(\n",
        "            by=[\"n_samples\", \"n_effective\"], ascending=False, inplace=False\n",
        "        ).reset_index(inplace=False, drop=True)\n",
        "        row = {\"eventname\": event_dir.name} | df.iloc[0].to_dict()\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows), failed_events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dense bank results for Figure 5 analysis\n",
        "# Note: This may take several minutes to process all dense bank results\n",
        "\n",
        "# First try to load from saved files\n",
        "dense_banks_dfs = []\n",
        "try:\n",
        "    for dense_bank_name in dense_bank_names:\n",
        "        df = pd.read_feather(\n",
        "            f\"{dense_bank_name}_results_for_ess_n_int_prime_relation.feather\"\n",
        "        )\n",
        "        dense_banks_dfs.append(df)\n",
        "    print(\"Loaded dense bank results from saved files\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Processing dense bank results from scratch...\")\n",
        "    # Process dense bank results from scratch\n",
        "    _t = time.time()\n",
        "    \n",
        "    failed_events = []\n",
        "    for dense_bank_name in dense_bank_names:\n",
        "        print(f\"Processing {dense_bank_name}...\")\n",
        "        df, _failed_events = load_bank_results_dense(f\"../{dense_bank_name}\")\n",
        "        failed_events.append(_failed_events)\n",
        "        df[[\"mchirp_005\", \"mchirp_095\"]] = pd.DataFrame(\n",
        "            df[\"rundir\"].apply(get_mchirp_quantiles).tolist(),\n",
        "            columns=[\"mchirp_005\", \"mchirp_095\"],\n",
        "        )\n",
        "        \n",
        "        # Get bank configuration for mass range\n",
        "        bank_config = utils.read_json(\n",
        "            banks_homedir / dense_bank_name / \"bank_config.json\"\n",
        "        )\n",
        "        min_mchirp = bank_config.get(\"min_mchirp\")\n",
        "        max_mchirp = bank_config.get(\"max_mchirp\")\n",
        "        df[\"integral\"] = np.log(df[\"mchirp_095\"] / df[\"mchirp_005\"]) / np.log(\n",
        "            max_mchirp / min_mchirp\n",
        "        )\n",
        "        df[\"log10_n_int_prime\"] = df[\"n_int_prime\"].apply(np.log10)\n",
        "        df[\"log10_n_effective_i\"] = df[\"n_effective_i\"].apply(np.log10)\n",
        "        df[\"log10_integral\"] = df[\"integral\"].apply(np.log10)\n",
        "        dense_banks_dfs.append(df)\n",
        "    \n",
        "    # Save results for future use\n",
        "    for df, dense_bank_name in zip(dense_banks_dfs, dense_bank_names):\n",
        "        df.to_feather(\n",
        "            f\"{dense_bank_name}_results_for_ess_n_int_prime_relation.feather\"\n",
        "        )\n",
        "    \n",
        "    _t = time.time() - _t\n",
        "    print(f\"Processing completed in {_t:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add derived quantities needed for Figure 5\n",
        "for df in dense_banks_dfs:\n",
        "    df[\"log10_n_int_prime_fraction\"] = np.log10(\n",
        "        df[\"n_int_prime\"] / df[\"n_int\"]\n",
        "    )\n",
        "    df[\"log10_efficiency\"] = np.log10(df[\"n_effective_i\"] / df[\"n_int\"])\n",
        "\n",
        "print(\"Dense bank processing complete\")\n",
        "print(\"Sample counts per bank:\")\n",
        "for dense_bank_name, df in zip(dense_bank_names, dense_banks_dfs):\n",
        "    print(f\"{dense_bank_name}: {len(df.eventname.unique())} events\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure 5: Corner plot showing the relationship between N'_int, N_eff_int, and M CI integral\n",
        "# Create labels with event counts\n",
        "labels = [\n",
        "    label + f\" ({len(df.eventname.unique())}/1024)\"\n",
        "    for label, df in zip(bank_labels.values(), dense_banks_dfs)\n",
        "]\n",
        "\n",
        "# Define parameters for the corner plot\n",
        "params = [\n",
        "    \"log10_integral\",\n",
        "    \"log10_efficiency\", \n",
        "    \"log10_n_int_prime_fraction\",\n",
        "]\n",
        "\n",
        "# Create the MultiCornerPlot\n",
        "mcp = gw_plotting.MultiCornerPlot(\n",
        "    dense_banks_dfs,\n",
        "    params=params,\n",
        "    labels=labels,\n",
        "    smooth=1,\n",
        ")\n",
        "\n",
        "# Define latex labels for the parameters\n",
        "latex_label = {\n",
        "    \"log10_n_int_prime_fraction\": r\"$\\log_{10}\\left[ {N_{\\rm int.}'} / {N_{\\rm int.}} \\right]$\",\n",
        "    \"log10_n_effective_i\": r\"$\\log_{10}$ Intrinsic ESS\",\n",
        "    \"log10_integral\": r\"$\\log_{10} \\left[\\int_{\\mathcal{M}_{5\\%}}^{\\mathcal{M}_{95\\%}} \\pi'(\\mathcal{M}) {\\rm d}\\mathcal{M} \\right]$\",\n",
        "    \"log10_efficiency\": r\"$\\log_{10}\\left[{N_{\\rm eff. int.}}/{N_{\\rm int.}}\\right]$\",\n",
        "}\n",
        "\n",
        "for cp in mcp.corner_plots:\n",
        "    cp.latex_labels |= latex_label\n",
        "\n",
        "mcp.plot()\n",
        "\n",
        "# Add 1:1 lines to specific panels\n",
        "fig = plt.gcf()\n",
        "# Add diagonal lines to panels [3, 6, 7] (based on the original code)\n",
        "for i in [3, 6, 7]:\n",
        "    ax = fig.axes[i]\n",
        "    x = ax.get_xlim()\n",
        "    ax.plot(x, x, ls=\"-\", c=\"k\")\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"selection_and_ESS.pdf\", format=\"pdf\", dpi=300)\n",
        "\n",
        "print(\"Figure 5: Selection and ESS analysis complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure 6: PP-Plot Analysis\n",
        "\n",
        "Now we recreate Figure 6 from the paper, showing probability-probability plots that validate the statistical properties of our posterior estimates across different mass ranges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PP-plot helper functions and parameters\n",
        "PARAMS_FOR_PP_PLOT = [\n",
        "    \"mchirp\",\n",
        "    \"q\", \n",
        "    \"chieff\",\n",
        "    \"cums1r_s1z\",\n",
        "    \"cums2r_s2z\",\n",
        "    \"ra\",\n",
        "    \"dec\",\n",
        "    \"d_luminosity\",\n",
        "]\n",
        "\n",
        "def get_cumsr_sz(sx, sy, sz):\n",
        "    s_perp_sqrt = sx**2 + sy**2\n",
        "    return s_perp_sqrt / (1 - sz**2)\n",
        "\n",
        "def get_cums1r_s1z_cums2r_s2z(pars):\n",
        "    cums1r_s1z = get_cumsr_sz(pars[\"s1x_n\"], pars[\"s1y_n\"], pars[\"s1z\"])\n",
        "    cums2r_s2z = get_cumsr_sz(pars[\"s2x_n\"], pars[\"s2y_n\"], pars[\"s2z\"])\n",
        "    return cums1r_s1z, cums2r_s2z\n",
        "\n",
        "def get_credible_intervals(hh_min, hh_max, rundirs, params=None, min_required_samples=30):\n",
        "    \"\"\"\n",
        "    Compute credible interval at which the injected value is recovered\n",
        "    for multiple parameters and injections.\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = PARAMS_FOR_PP_PLOT\n",
        "    \n",
        "    credible_intervals = {par: [] for par in params}\n",
        "    \n",
        "    for rundir in tqdm(rundirs):\n",
        "        rundir = Path(rundir)\n",
        "        samples_path = rundir / \"samples.feather\"\n",
        "        injection_parameters_path = rundir / \"injection_parameters.json\"\n",
        "        \n",
        "        if not (samples_path.exists() and injection_parameters_path.exists()):\n",
        "            for par in params:\n",
        "                credible_intervals[par].append(np.nan)\n",
        "            continue\n",
        "            \n",
        "        samples = pd.read_feather(samples_path)\n",
        "        \n",
        "        # Filter samples by SNR range\n",
        "        if \"hh\" in samples.columns:\n",
        "            valid_samples = samples[(samples[\"hh\"] >= hh_min) & (samples[\"hh\"] <= hh_max)]\n",
        "        else:\n",
        "            valid_samples = samples\n",
        "            \n",
        "        if len(valid_samples) < min_required_samples:\n",
        "            print(f\"Skipping {rundir} with only {len(valid_samples)} samples with ⟨h∣h⟩ in ({hh_min},{hh_max}).\")\n",
        "            for par in params:\n",
        "                credible_intervals[par].append(np.nan)\n",
        "            continue\n",
        "        \n",
        "        injection_pars = utils.read_json(injection_parameters_path)\n",
        "        \n",
        "        # Add derived parameters\n",
        "        if \"cums1r_s1z\" in params or \"cums2r_s2z\" in params:\n",
        "            if all(k in injection_pars for k in [\"s1x_n\", \"s1y_n\", \"s1z\", \"s2x_n\", \"s2y_n\", \"s2z\"]):\n",
        "                cums1r_s1z, cums2r_s2z = get_cums1r_s1z_cums2r_s2z(injection_pars)\n",
        "                injection_pars[\"cums1r_s1z\"] = cums1r_s1z\n",
        "                injection_pars[\"cums2r_s2z\"] = cums2r_s2z\n",
        "                \n",
        "                cums1r_s1z_samples, cums2r_s2z_samples = get_cums1r_s1z_cums2r_s2z(valid_samples)\n",
        "                valid_samples[\"cums1r_s1z\"] = cums1r_s1z_samples\n",
        "                valid_samples[\"cums2r_s2z\"] = cums2r_s2z_samples\n",
        "        \n",
        "        for par in params:\n",
        "            if par in injection_pars and par in valid_samples.columns:\n",
        "                injected_value = injection_pars[par]\n",
        "                parameter_samples = valid_samples[par].values\n",
        "                \n",
        "                # Calculate credible interval\n",
        "                credible_interval = np.mean(parameter_samples <= injected_value)\n",
        "                credible_intervals[par].append(credible_interval)\n",
        "            else:\n",
        "                credible_intervals[par].append(np.nan)\n",
        "    \n",
        "    return pd.DataFrame(credible_intervals)\n",
        "\n",
        "def pp_plot(credible_intervals, params=None, ax=None, show_xy_labels=True, show_title=True, show_legend=True):\n",
        "    \"\"\"\n",
        "    Make a probability-probability plot.\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots()\n",
        "\n",
        "    if params is None:\n",
        "        params = list(credible_intervals)\n",
        "\n",
        "    clean_credible_intervals = credible_intervals.dropna()\n",
        "    for par in params:\n",
        "        sorted_credible_intervals = np.sort(clean_credible_intervals[par])\n",
        "        ax.plot(\n",
        "            sorted_credible_intervals,\n",
        "            np.linspace(0, 1, len(clean_credible_intervals)),\n",
        "            label=gw_plotting.CornerPlot.DEFAULT_LATEX_LABELS.get(par, par),\n",
        "            lw=1,\n",
        "        )\n",
        "    ax.plot((0, 1), (0, 1), \"k:\")  # Diagonal line\n",
        "\n",
        "    if show_title:\n",
        "        ax.set_title(\n",
        "            f\"$N = {len(clean_credible_intervals)} / {len(credible_intervals)}$\",\n",
        "            fontsize=\"medium\",\n",
        "        )\n",
        "\n",
        "    ax.tick_params(axis=\"x\", direction=\"in\", top=True)\n",
        "    ax.tick_params(axis=\"y\", direction=\"in\", right=True)\n",
        "    ax.grid(linestyle=\":\")\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    if show_legend:\n",
        "        ax.legend(\n",
        "            fontsize=10,\n",
        "            frameon=True,\n",
        "            framealpha=0.5,\n",
        "            labelspacing=0.25,\n",
        "            loc=\"upper left\",\n",
        "            edgecolor=\"none\",\n",
        "            borderpad=0.3,\n",
        "        )\n",
        "\n",
        "    if show_xy_labels:\n",
        "        ax.set_xlabel(\"Credible interval\")\n",
        "        ax.set_ylabel(\"Fraction of injections in credible interval\")\n",
        "\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate PP-plots using dense bank results\n",
        "# Note: This uses the same dense bank data as Figure 5\n",
        "\n",
        "print(\"Computing credible intervals for PP-plot analysis...\")\n",
        "\n",
        "# Parameters for credible interval calculation  \n",
        "hh_min = 70\n",
        "hh_max = 200\n",
        "min_required_samples = 20\n",
        "\n",
        "# Compute credible intervals for each dense bank\n",
        "ci_per_bank = {}\n",
        "for i, dense_bank_name in enumerate(dense_bank_names):\n",
        "    print(f\"Processing {dense_bank_name}...\")\n",
        "    \n",
        "    # Get rundirs from the dense bank dataframe we already have\n",
        "    df = dense_banks_dfs[i]\n",
        "    rundirs = [Path(rundir) for rundir in df[\"rundir\"].values.tolist()]\n",
        "    \n",
        "    # Compute credible intervals\n",
        "    ci_per_bank[dense_bank_name] = get_credible_intervals(\n",
        "        hh_min,\n",
        "        hh_max, \n",
        "        rundirs,\n",
        "        params=PARAMS_FOR_PP_PLOT,\n",
        "        min_required_samples=min_required_samples,\n",
        "    )\n",
        "\n",
        "print(\"Credible interval computation complete\")\n",
        "for k, v in ci_per_bank.items():\n",
        "    print(f\"{k}: {len(v)} total, {len(v.dropna())} valid\")\n",
        "\n",
        "# Get bank configurations for subplot titles\n",
        "config_paths = [\n",
        "    banks_homedir / dense_bank_name / \"bank_config.json\"\n",
        "    for dense_bank_name in dense_bank_names\n",
        "]\n",
        "\n",
        "bank_configs_dense = {\n",
        "    dense_bank_name: utils.read_json(config_path)\n",
        "    for dense_bank_name, config_path in zip(dense_bank_names, config_paths)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Figure 6: PP-plots in 2x3 grid\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10), sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, (dense_bank_name, credible_intervals) in zip(axes, ci_per_bank.items()):\n",
        "    # Show legend only on first subplot\n",
        "    show_legend = dense_bank_name == dense_bank_names[0]\n",
        "    \n",
        "    # Create PP-plot\n",
        "    pp_plot(\n",
        "        credible_intervals,\n",
        "        ax=ax,\n",
        "        show_legend=show_legend,\n",
        "        show_xy_labels=True,\n",
        "    )\n",
        "    \n",
        "    # Add subplot title with mass range\n",
        "    mmin = bank_configs_dense[dense_bank_name][\"min_mchirp\"]\n",
        "    mmax = bank_configs_dense[dense_bank_name][\"max_mchirp\"]\n",
        "    \n",
        "    title = (\n",
        "        r\"$\\mathcal{M}^{\\rm det}\\in(\"\n",
        "        + f\"{mmin:.3g},{mmax:.3g}\"\n",
        "        + r\"){\\rm M}_{\\odot}$, \"\n",
        "    )\n",
        "    \n",
        "    # Extract sample count from current title and add to new title\n",
        "    current_title = ax.get_title()\n",
        "    sample_count = current_title.split(\"/\")[0] if \"/\" in current_title else \"N\"\n",
        "    ax.set_title(title + sample_count + r\"/1024$\", fontsize=14)\n",
        "\n",
        "    # Adjust legend font size for first subplot\n",
        "    if show_legend:\n",
        "        for text in ax.get_legend().get_texts():\n",
        "            text.set_fontsize(12)\n",
        "\n",
        "# Set common axis labels\n",
        "for ax in axes:\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_ylabel(\"\")\n",
        "\n",
        "# Set row labels\n",
        "axes[0].set_ylabel(\"Fraction of injections in credible interval\", fontsize=14)\n",
        "axes[3].set_ylabel(\"Fraction of injections in credible interval\", fontsize=14)\n",
        "\n",
        "# Set column labels\n",
        "axes[1].set_xlabel(\"Credible interval\", fontsize=14)\n",
        "axes[4].set_xlabel(\"Credible interval\", fontsize=14)\n",
        "\n",
        "# Adjust font sizes\n",
        "for ax in axes:\n",
        "    ax.set_title(ax.get_title(), fontsize=14)\n",
        "    for label in ax.get_xticklabels():\n",
        "        label.set_fontsize(12)\n",
        "    for label in ax.get_yticklabels():\n",
        "        label.set_fontsize(12)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"pp_plots.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
        "\n",
        "print(\"Figure 6: PP-plot analysis complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
